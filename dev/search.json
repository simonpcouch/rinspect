[{"path":[]},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://vitals.tidyverse.org/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://vitals.tidyverse.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 vitals authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"data-structure","dir":"Articles","previous_headings":"","what":"Data Structure","title":"Analyzing evaluation results","text":"Revisiting are_eval:  data: ’re comparing results 3 different tasks. separate tasks, case, represent three different LLMs, Anthropic’s Claude 4 Sonnet, OpenAI’s GPT 4.1, Google’s Gemini 2.5 Pro. ’ll refer identifier task p (shown are_eval column LLM). Underlying tasks dataset n=n = 75 samples. sample contains question asked LLM. LLM answers given sample, answer assigned one three qualitative, ordinal scores: Incorrect (), Partially correct (P), correct (C). ’ll denote YY discrete random variable representing scores take values 1,2,…c1, 2, \\ldots c (c=3c=3 ). completely balanced data set contain p×np \\times n rows (case). data structure, LLM systematic effect data like make inferences (.e., accuracy better one LLM, etc.). samples independent experimental units; statistically, believe results sample 1 independent sample 2 . two potential levels hierarchy need consider analyzing evaluation data: LLMs deterministic; given prompt, LLMs always return answer. measure variation, run sample multiple times, generating set mmepochs. thus must account repeated measures, epochs systematic ordering effect. However, expect within-sample correlation occur. means expect mm data points within specific sample likely correlate one another data point different sample. Second, rating method variable. Multiple raters might score LLM output multiple samples (sample/epoch combination) offset chances systematic rater bias. also result similar within-rater correlation. case, single rater, don’t need account specific analysis. primary data layer case independent experimental unit: samples. layer incurs additional variance component. make valid inferences, crucial appropriately account multiple sources variation. Consider random variables AA BB (, context, might represent accuracies LLMs) simple example. wanted evaluate difference, variance difference Var[−B]=Var[]+Var[B]−2Cov[,B]Var[-B] = Var[] + Var[B] - 2Cov[,B]. Standard statistical tools, like basic t-test, use signal--noise ratios. covariance term nonzero within-sample (rater) correlation. ignore , artificially -power assessment difference might erroneously fail show difference. analyses differentiate cases one data hierarchies.","code":"are_eval |>   mutate(     score = factor(       case_when(         score == \"I\" ~ \"Incorrect\",         score == \"P\" ~ \"Partially correct\",         score == \"C\" ~ \"Correct\"       ),       # align ordering with original factor       levels = c(\"Incorrect\", \"Partially correct\", \"Correct\"),       ordered = TRUE     )   ) |>   ggplot(aes(y = LLM, fill = score)) +   geom_bar(alpha = 2 / 3) +   scale_fill_brewer(palette = \"Set1\")"},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"anova-type-models","dir":"Articles","previous_headings":"","what":"ANOVA-Type Models","title":"Analyzing evaluation results","text":"analysis variance (ANOVA) bedrock tool comparing different groups data set. outcome scores categories, type model called generalized linear model can used analyze results. Without ordered categories, ’ll focus ordinal logistic regression model. many different variants model one common cumulative probability model. k=1…ck=1\\ldots c possible values scores, c−1c-1 probabilities Pr[Yi≥k]Pr[Y_{} \\ge k] =1…ni=1\\ldots n samples. example, Pr[Yi≥2]Pr[Y_{} \\ge 2] model probability completely incorrect.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"independent-data","dir":"Articles","previous_headings":"ANOVA-Type Models","what":"Independent Data","title":"Analyzing evaluation results","text":"Let’s first consider case rows are_eval statistically independent, conditional LLM. Said another way, can keep rows first epoch: setting, visualize scores sample--sample one epoch like :  model score probabilities, cumulative logit model estimates c−1c-1 values log(Pr[Yi≥k]1−Pr[Yi≥k]])=θk−(β2xi2+…+βpxip) \\log\\left(\\frac{Pr[Y_{} \\ge k]}{1 - Pr[Y_{} \\ge k]]}\\right) = \\theta_k - (\\beta_2x_{i2} + \\ldots +  \\beta_{p}x_{ip}) =1…ni=1\\ldots n samples evaluated j=2…pj=2\\ldots p LLMs produce outcomes k=1…c−1k=1\\ldots c-1. xijx_{ij} values binary indicators one LLMs. full-rank parameterization; effect LLM corresponding j=1j=1 captured different intercepts (θk\\theta_k). parameterization, βj\\beta_j parameters correspond effect LLM jj beyond effect LLM associated j=1j=1. parallel model assumes pattern LLM effectiveness constant across different outcome categories; words, Model twice likely Model B get “Correct” vs “Partially correct” answer, Model also twice likely get “Partially correct Correct” vs “Incorrect”. can also referred proportional odds model. parameters can estimated using standard maximum likelihood estimation, assuming multinomial distribution. enables us easily calculate covariance matrix parameters inferential procedures. properties maximum likelihood, can also make inferences functions parameters monotonic nature (e.g., log(β)log(\\beta), etc). ’ve ordered LLMs worst-performing LLM index j=1j=1.1 parameterization allows direct path inference since exp(βj)exp(\\beta_j) parameters increase odds LLM jj performs better worst LLM. can easily estimate null hypotheses test two LLMs’ differences /create confidence intervals odds. model structure simple one-epoch case: estimates follows: first point inference determine difference LLMs. determined fitting submodel β\\beta parameters. difference likelihood values models can compared appropriate χ2\\chi^2 distribution produce p-value null hypothesis βj\\beta_j values equal. analysis excludes one epoch, p-value 0.861, indicating evidence models different accuracies data. Despite , let’s look coefficients can interpreted. table shows exponentiated coefficients 90% confidence intervals. Maximum likelihood estimation enables use monotonic transformations estimates. odds ratios different LLMs. Recall , model fit, LLMs indexed GPT 4.1 first position. estimates increase odds correct GPT 4.1. fact intervals include 1.0 indicates enough evidence believe differences LLMs.","code":"are_eval_single <- are_eval %>% filter(epoch == 1)  are_eval_single ## # A tibble: 75 × 5 ##    LLM             id                          epoch score metadata ##    <fct>           <chr>                       <int> <ord> <list>   ##  1 Claude 4 Sonnet after-stat-bar-heights          1 I     <tibble> ##  2 Claude 4 Sonnet conditional-grouped-summary     1 C     <tibble> ##  3 Claude 4 Sonnet correlated-delays-reasoning     1 P     <tibble> ##  4 Claude 4 Sonnet curl-http-get                   1 I     <tibble> ##  5 Claude 4 Sonnet dropped-level-legend            1 I     <tibble> ##  6 Claude 4 Sonnet geocode-req-perform             1 C     <tibble> ##  7 Claude 4 Sonnet grouped-filter-summarize        1 P     <tibble> ##  8 Claude 4 Sonnet grouped-mutate                  1 C     <tibble> ##  9 Claude 4 Sonnet implement-nse-arg               1 P     <tibble> ## 10 Claude 4 Sonnet jitter-one-axis                 1 C     <tibble> ## # ℹ 65 more rows are_eval_single %>%   ggplot(aes(x = LLM, y = id)) +   geom_raster(aes(fill = score), alpha = 2 / 3) +   scale_fill_brewer(palette = \"Set1\") +   labs(x = NULL, y = NULL) +   theme(legend.position = \"top\") single_mod <- clm(score ~ LLM, data = are_eval_single) tidy(single_mod, conf.int = TRUE, conf.level = 0.9) %>%   mutate(     LLM = gsub(\"LLM\", \"\", term),     parameter = ifelse(coef.type == \"intercept\", \"theta\", \"beta\")   ) single_coef %>%    filter(!grepl(\"\\\\|\", LLM)) %>%    select(LLM, estimate, lower = conf.low, upper = conf.high) %>%    mutate(     estimate = exp(estimate),     lower = exp(lower),     upper = exp(upper)   )"},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"multiple-hierarchies","dir":"Articles","previous_headings":"ANOVA-Type Models","what":"Multiple Hierarchies","title":"Analyzing evaluation results","text":"analysis change incorporate samples every epoch? illustrate structure ’d like model, can visualize variation scores sample LLM similarly :  can extend cumulative logistic model account additional data hierarchies incorporating random effects due specific random variables. case are_eval, ’ve evaluated model epochs = 3, three replicates sample. interested making inferential statements specific replicates; nuisance variables must account analyses adjacent goals. common approach add random effect model , average, zero effect model’s prediction function. Instead, helps model variability data. Let’s adjust model equation add random intercept term due replicates: log(Pr[Yi≥k]1−Pr[Yi≥k]])=(θk−αℓ)−(β2xi2+…+βpxip) \\log\\left(\\frac{Pr[Y_{} \\ge k]}{1 - Pr[Y_{} \\ge k]]}\\right) = (\\theta_k - \\alpha_{\\ell})- (\\beta_2x_{i2} + \\ldots +  \\beta_{p}x_{ip}) αℓ\\alpha_{\\ell} reflects random variation intercept category due replicate ℓ=1…m\\ell=1\\ldots m replicates. need make probabilistic assumption values, convenient approach assume independently distributed αℓ∼N(0,σ2)\\alpha_{\\ell} \\sim N(0, \\sigma^2). addition model reflects baseline log-odds can randomly fluctuate different epochs. Still, overall relationship log odds LLM parameters change (average). complex estimation method needed solve maximum likelihood equations. However, model shown overly complex , two epochs, difficult train. can still conduct inferential methods previously shown, , bonus, can use estimates αℓ\\alpha_{\\ell} help us understand samples difficult LLMs correctly solve. model structure, first fit two models. first , additionally incorporating random effect (1|id). Note random effect question ID rather epoch. second, null model account LLM. Comparing two model fits, , shows us effect incorporating LLM variable fit: case, increased number data points results much smaller p-value overall test difference LLM (0.273). Notice parameter estimates larger magnitude, standard errors fairly small relative point estimates. analysis, odds ratios indicate two models improved accuracy relative GPT 4.1, overlap 90% confidence intervals 1 suggests statistically significant differences LLMs. Running evaluation across multiple epochs also allows us ask interesting questions evaluation (rather LLMs’ performance alone), . questions harder others? Since included (1|id) model structure, model estimates random effects α̂ℓ\\widehat{\\alpha}_{\\ell} question.  distribution assumed Gaussian-like, smaller values indicating sample difficult. example, challenging sample “-stat-bar-heights” , across LLMs epochs, 0 9 scores categorized correct.","code":"are_eval %>%   ggplot(aes(x = epoch, y = id)) +   geom_raster(aes(fill = score), alpha = 2 / 3) +   facet_wrap(~ LLM, ncol = 3) +   scale_fill_brewer(palette = \"Set1\") +   labs(x = \"Epoch\", y = NULL) +   theme(     legend.position = \"top\",     strip.text = element_text(size = 8)   ) multiple_mod <- clmm(score ~ LLM + (1|id), data = are_eval, Hess = TRUE) multiple_null <- clmm(score ~ 1 + (1|id), data = are_eval, Hess = TRUE) multiple_lrt <- anova(multiple_null, multiple_mod)  multiple_coef <-    multiple_mod %>%    tidy(conf.int = TRUE, conf.level = 0.9) %>%    mutate(     LLM = gsub(\"LLM\", \"\", term),     parameter = ifelse(coef.type == \"intercept\", \"theta\", \"beta\")   ) multiple_intercepts <-    are_eval %>%    distinct(id) %>%   mutate(     effect = multiple_mod$ranef,     # reorder the ids by the magnitude of the effect associated with them     id = factor(id),     id = reorder(id, effect)   ) effect_range <- max(extendrange(abs(multiple_intercepts$effect)))  multiple_intercepts %>%    ggplot(aes(x = effect, y = id)) +    geom_point() +   labs(y = NULL, x = \"Random Intercept Estimate\\n(incorrect <----------------> correct)\") +   lims(x = c(-effect_range, effect_range))"},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"bayesian-hierarchical-models","dir":"Articles","previous_headings":"","what":"Bayesian Hierarchical Models","title":"Analyzing evaluation results","text":"Previously, estimated parameters cumulative logit model using maximum likelihood. assumed binomial likelihood outcome, case additional hierarchies, assumed normality random effects. make probabilistic assumptions, can maximize likelihood function determine estimates θ̂\\widehat{\\theta}, β̂\\widehat{\\beta}, potentially, α̂\\widehat{\\alpha} parameters. MLE finds point estimates parameters maximize probability training set occurred. can use Bayesian estimation fit model. technique also uses likelihood blends prior knowledge parameters. example, previously assumed random effects replicates normally distributed. believe results samples distribution? example, thought symmetric distribution still appropriate suspected samples abnormally easy difficult solve? might lead us symmetric distribution like t, heavier tails. Alternatively, skewed distribution appropriate corpus samples mostly easy exceedingly difficult. Bayesian estimation facilitate assumptions. also lets us make natural straightforward statements regarding parameters. Instead convoluted interpretation confidence intervals, Bayesian methods allow us create simple direct probabilistic statements regarding function one parameters. downside using models numerically complex computationally demanding. Unlike maximum likelihood estimation, seeking solve specific parameter estimate multivariate distribution estimates. Additionally, diagnostic tools understand training algorithm converged trivial. said, let’s train previous random effects model using Bayesian estimation. little priori knowledge regression parameters (β\\beta parameters) primary intercepts (θ\\theta), can assign wide Gaussian prior distributions. Assigning large variance allows prior beliefs also allows observed data influence model estimates . ’ll assume wide t distribution via αℓ∼t(1)\\alpha_{\\ell} \\sim t(1) random effects. slightly informative distributional assumption one used previous random effects model. fitting algorithm uses Markov chain Monte Carlo (MCMC) selectively sample overall parameter distribution convergence. ’ll run algorithm 10 times parallel, 10,000 iterations. first half iterations used estimate posterior distribution, remaining samples used approximate distribution. Formatting results: model results fitted distributions parameter (rather single estimated value). posterior distributions regression model parameters can visualized:  Since parameters measure LLM effectiveness (relative baseline LLM GPT 4.1), can tell model thinks likely values . suggests GPT 4.1 performs par Gemini 2.5 Pro slightly worse Claude 4 Sonnet. Bayesian analysis take worldview null hypothesis-driven inference, p-value report. can see figure small area posterior distributions near zero. example, NA% Claude 4 Sonnet posterior within ±0.05\\pm 0.05. can quantify posteriors using standard summary statistics: percentile columns credible intervals allow us make statements : believe true odds ratio comparing GPT 4.1 Claude 4 Sonnet 90% probability falling 0.302 1.09. much straightforward definition confidence intervals. Additionally, model produces posterior distributions random effects. can repeat earlier analysis rank difficulty sample along 90% credible intervals aid interpretability:","code":"multiple_bayes <-   brm(     score ~ LLM + (1|id),     data = are_eval,     family = cumulative(link = \"logit\", threshold = \"flexible\"),     prior = c(set_prior(prior = \"student_t(1, 0, 1)\", class = \"Intercept\")),     chains = 10,     iter = 10000,     cores = 10,     seed = 410   ) set.seed(280)  all_post <- as_draws_df(multiple_bayes)  bayes_regression_param <-    all_post %>%   select(contains(\"b_LLM\")) %>%    pivot_longer(     cols = c(everything()),     names_to = \"param_name\",     values_to = \"value\"   ) %>%    mutate(     LLM =        case_when(         param_name == \"b_LLMGemini2.5Pro\" ~ \"Gemini 2.5 Pro\",         param_name == \"b_LLMClaude4Sonnet\" ~ \"Claude 4 Sonnet\"       ),     LLM = factor(LLM)   )   bayes_reg_summary <-    bayes_regression_param %>%    summarize(     mean = mean(value),     lower = quantile(value, 0.05),     upper = quantile(value, 0.95),     mean_odds = mean(exp(value)),     lower_odds = quantile(exp(value), 0.05),     upper_odds = quantile(exp(value), 0.95),     .by = c(LLM)   ) post_claude <- bayes_regression_param$value[bayes_regression_param$LLM == \"Claude 4 Sonnet\"] o2_le_zero <- mean(abs(post_claude) < 0.05) * 100  bayes_regression_param %>%    ggplot(aes(x = value)) +    geom_vline(xintercept = 0, lty = 2, col = \"red\") +   geom_density(aes(col = LLM), linewidth = 1, alpha = 1 / 2) +   labs(x = expression(hat(beta))) +   theme(legend.position = \"top\") +    scale_color_brewer(palette = \"Set2\") bayes_reg_summary bayes_intercepts <-    all_post %>%   select(contains(\"r_id\")) %>%   pivot_longer(     cols = c(everything()),     names_to = \"sample\",     values_to = \"value\"   ) %>%   summarize(     mean = mean(value),     lower = quantile(value, prob = 0.05),     upper = quantile(value, prob = 0.95),     .by = c(sample)   ) %>%   mutate(     id = gsub(\",Intercept]\", \"\", sample, fixed = TRUE),     id = gsub(\"r_id[\", \"\", id, fixed = TRUE),     id = factor(id)   ) %>%   arrange(id) %>%    inner_join(     are_eval %>% distinct(id), by = \"id\"   ) %>%    mutate(     id = as.character(id),     id = reorder(id, mean)   )  bayes_effect_range <-    bayes_intercepts %>%    select(lower, upper) %>%    unlist() %>%    abs() %>%    extendrange() %>%    max()  bayes_intercepts %>%    ggplot(aes(y = id)) +   geom_point(aes(x = mean)) +   geom_errorbar(aes(xmin = lower, xmax = upper), width = 1 / 2) +   labs(y = NULL, x = \"Random Intercept Estimate\\n(incorrect <----------------> correct)\") +   lims(x = c(-bayes_effect_range, bayes_effect_range))"},{"path":"https://vitals.tidyverse.org/dev/articles/analysis.html","id":"extendability","dir":"Articles","previous_headings":"","what":"Extendability","title":"Analyzing evaluation results","text":"Using statistical regression model qualitative scores enables significant amount flexibility makes robust experimental design changes. Adding new LLMs trivial since model specific limit regression parameters. Additionally, may need analyze systematic effects data, : Comparing various system prompts Evaluating multiple versions parameter settings given LLM. . can also add quantitative covariates, allowed number characters LLM response queries. particular, Bayesian approach flexibility terms (many) random effects required accommodate data hierarchies experimental design.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/solvers.html","id":"an-r-eval-dataset","dir":"Articles","previous_headings":"","what":"An R eval dataset","title":"Custom solvers and tool calling","text":"docs: R Eval dataset challenging R coding problems. input question R code solved first-read human experts , chance read documentation run code, fluent data scientists. Solutions target enable fluent data scientist evaluate whether solution deserves full, partial, credit. high level: id: unique identifier problem. input: question answered. target: solution, often description notable features correct solution. domain, task, knowledge pieces metadata describing kind R coding challenge. source: problem came , URL. Many coding problems adapted “wild” include kinds context usually available answering questions. introductory vignette, just provided inputs -ellmer chat’s $chat() method (via built-generate() solver). vignette, ’ll compare results approach resulting first giving models chance read relevant R package documentation.","code":"glimpse(are) #> Rows: 29 #> Columns: 7 #> $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… #> $ input     <chr> \"This bar chart shows the count of different cuts o… #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t…"},{"path":"https://vitals.tidyverse.org/dev/articles/solvers.html","id":"custom-solvers","dir":"Articles","previous_headings":"","what":"Custom solvers","title":"Custom solvers and tool calling","text":"help us better understand solvers work, lets look implementation generate(). generate() function factory, meaning function outputs function. , documentation, ’ve mostly written “generate() just calls $chat(),” couple extra tricks generate()’s sleeves: chat first cloned $clone() original chat object isn’t modified vitals calls . Rather $chat(), generate() calls ellmer::parallel_chat(). generate() takes inputs–e.g. whole input column–rather single input. parallel_chat() send requests parallel (usual R sense multiple R processes, sense asychronous HTTP requests), greatly reducing amount time takes run evaluations. function generate outputs takes list inputs outputs list two elements: result final result solver. case, purrr::map_chr(res, function(c) c$last_turn()@text) just grabs text last assistant run returns . result vector length equal inputs—able tacked onto task$get_samples() another column. solver_chat list chats led result. may seem redundant result, important supply vitals package can log happened solving process display log viewer. work slightly modified version generate() equips models ability peruse R package documentation answering. using btw package, available GitHub https://posit-dev.github.io/btw/. Among things, btw provides function btw_tools() returns list tools can registered ellmer chat. can use chat’s $chat() method interact like . example, registering btw Claude Sonnet 3.7 chat object, can ask package, vitals. time writing, vitals CRAN, Claude Sonnet 3.7 completed training several months started working . Nonetheless, model able tell custom solvers work: ’ll help find information custom solvers vitals package. Let first check vitals package installed explore documentation. models lets us know ’s reading bunch documentation.<> create custom solver, need : Takes vector inputs first argument result: vector final responses (length input) solver_chat: list ellmer chat objects used (length input) Preprocess inputs sending LLM Post-process model responses Implement special handling like multi-turn conversations Incorporate tools external knowledge Add retry mechanisms fallbacks Yada yada yada. Pretty spot-. capability help models better answer questions R? built questions base R popular packages like dplyr shiny, information presumably baked weights. implementation custom solver looks almost exactly like generate()s implementation, except add call btw_tools() calling ellmer::parallel_chat(). solver ready situate Task get solving!","code":"sonnet_3_7 <- chat_anthropic(model = \"claude-3-7-sonnet-latest\") generate(solver_chat = sonnet_3_7$clone()) #> function (inputs, ..., solver_chat = chat)  #> { #>     check_inherits(solver_chat, \"Chat\") #>     ch <- solver_chat$clone() #>     res <- ellmer::parallel_chat(ch, as.list(inputs), ...) #>     list(result = purrr::map_chr(res, function(c) c$last_turn()@text),  #>         solver_chat = res) #> } #> <bytecode: 0x561ec7bfca80> #> <environment: 0x561ec7bfb9a8> ch <- sonnet_3_7$clone() ch$set_tools(btw_tools(tools = \"docs\")) ✔ Registered tool btw_tool_docs_package_help_topics ✔ Registered tool btw_tool_docs_help_page ✔ Registered tool btw_tool_docs_available_vignettes ✔ Registered tool btw_tool_docs_vignette ch$chat(\"How do custom solvers work in vitals?\") btw_solver <- function(inputs, ..., solver_chat) {   ch <- solver_chat$clone()   ch$set_tools(btw_tools(tools = \"docs\"))   res <- ellmer::parallel_chat(ch, as.list(inputs))   list(     result = purrr::map_chr(res, function(c) c$last_turn()@text),      solver_chat = res   ) }"},{"path":"https://vitals.tidyverse.org/dev/articles/solvers.html","id":"running-the-solver","dir":"Articles","previous_headings":"","what":"Running the solver","title":"Custom solvers and tool calling","text":"Situate custom solver dataset way built-one: introductory vignette, supply dataset built-scorer model_graded_qa(). , use $eval() evaluate custom solver, evaluate scorer, explore persistent log results interactive Inspect log viewer. arguments solving scoring functions can passed directly $eval(), allowing quickly evaluating tasks across several parameterizations. ’ll just use Claude Sonnet 3.7 now. Claude answered fully correctly 15 29 samples, partially correctly 8 times. Using metadata, can also determine often model looked least one help-page supplying answer: equipped ability peruse documentation, model almost always made use . see kinds docs decided look , click samples log viewer eval:  perusing documentation actually help model write better R code, though? order quantify “better,” need also run evaluation without model able access documentation. Using code introductory vignette: (Note also used are_btw$set_solver() didn’t want recreate task.) difference performance just result noise, though? can supply scores ordinal regression model answer question. coefficient solver == \"btw\" 0.216, indicating allowing model peruse documentation tends associated higher grades. 95% confidence interval coefficient contains zero, can conclude sufficient evidence reject null hypothesis difference GPT-4o Claude’s performance eval zero 0.05 significance level. evaluated model across multiple epochs, question ID become “nuisance parameter” mixed model, e.g. model structure ordinal::clmm(score ~ model + (1|id), ...). article demonstrated using custom solver modifying built-generate() allow models make tool calls peruse package documentation. vitals, tool calling “just works”; calls tool registered ellmer automatically incorporated evaluation logs.","code":"are_btw <- Task$new(   dataset = are,   solver = btw_solver,   scorer = model_graded_qa(partial_credit = TRUE, scorer_chat = sonnet_3_7$clone()),   name = \"An R Eval\" )  are_btw are_btw$eval(solver_chat = sonnet_3_7$clone()) are_btw_data <- vitals_bind(are_btw)  are_btw_data #> # A tibble: 29 × 4 #>    task    id                          score metadata          #>    <chr>   <chr>                       <ord> <list>            #>  1 are_btw after-stat-bar-heights      I     <tibble [1 × 10]> #>  2 are_btw conditional-grouped-summary C     <tibble [1 × 10]> #>  3 are_btw correlated-delays-reasoning P     <tibble [1 × 10]> #>  4 are_btw curl-http-get               P     <tibble [1 × 10]> #>  5 are_btw dropped-level-legend        I     <tibble [1 × 10]> #>  6 are_btw filter-multiple-conditions  C     <tibble [1 × 10]> #>  7 are_btw geocode-req-perform         C     <tibble [1 × 10]> #>  8 are_btw group-by-summarize-message  C     <tibble [1 × 10]> #>  9 are_btw grouped-filter-summarize    P     <tibble [1 × 10]> #> 10 are_btw grouped-geom-line           C     <tibble [1 × 10]> #> # ℹ 19 more rows chat_called_a_tool <- function(chat) {   turns <- chat[[1]]$get_turns()   any(purrr::map_lgl(turns, turn_called_a_tool)) }  turn_called_a_tool <- function(turn) {   any(purrr::map_lgl(turn@contents, inherits, \"ellmer::ContentToolRequest\")) }  are_btw_data |>   tidyr::hoist(metadata, \"solver_chat\") |>   mutate(called_a_tool = purrr::map_lgl(solver_chat, chat_called_a_tool)) |>   pull(called_a_tool) |>    table() #>  #> FALSE  TRUE  #>     3    26 are_basic <- Task$new(   dataset = are,   solver = generate(solver_chat = sonnet_3_7$clone()),   scorer = model_graded_qa(partial_credit = TRUE, scorer_chat = sonnet_3_7$clone()),   name = \"An R Eval\" )  are_basic$eval() are_eval_data <-    # the argument names will become the entries in `task`   vitals_bind(btw = are_btw, basic = are_basic) |>   # and, in this case, different tasks correspond to different solvers   rename(solver = task)  are_eval_data #> # A tibble: 58 × 4 #>    solver id                          score metadata          #>    <chr>  <chr>                       <ord> <list>            #>  1 btw    after-stat-bar-heights      I     <tibble [1 × 10]> #>  2 btw    conditional-grouped-summary C     <tibble [1 × 10]> #>  3 btw    correlated-delays-reasoning P     <tibble [1 × 10]> #>  4 btw    curl-http-get               P     <tibble [1 × 10]> #>  5 btw    dropped-level-legend        I     <tibble [1 × 10]> #>  6 btw    filter-multiple-conditions  C     <tibble [1 × 10]> #>  7 btw    geocode-req-perform         C     <tibble [1 × 10]> #>  8 btw    group-by-summarize-message  C     <tibble [1 × 10]> #>  9 btw    grouped-filter-summarize    P     <tibble [1 × 10]> #> 10 btw    grouped-geom-line           C     <tibble [1 × 10]> #> # ℹ 48 more rows library(ordinal) #>  #> Attaching package: 'ordinal' #> The following object is masked from 'package:dplyr': #>  #>     slice  are_mod <- clm(score ~ solver, data = are_eval_data)  are_mod #> formula: score ~ solver #> data:    are_eval_data #>  #>  link  threshold nobs logLik AIC    niter max.grad cond.H  #>  logit flexible  58   -60.19 126.38 4(0)  1.25e-09 1.2e+01 #>  #> Coefficients: #> solverbtw  #>    0.2159  #>  #> Threshold coefficients: #>     I|P     P|C  #> -1.0381  0.1101 confint(are_mod) #>                2.5 %   97.5 % #> solverbtw -0.7572417 1.196237"},{"path":"https://vitals.tidyverse.org/dev/articles/vitals.html","id":"an-r-eval-dataset","dir":"Articles","previous_headings":"","what":"An R eval dataset","title":"Getting started with vitals","text":"docs: R Eval dataset challenging R coding problems. input question R code solved first-read human experts , chance read documentation run code, fluent data scientists. Solutions target enable fluent data scientist evaluate whether solution deserves full, partial, credit. high level: id: unique identifier problem. input: question answered. target: solution, often description notable features correct solution. domain, task, knowledge pieces metadata describing kind R coding challenge. source: problem came , URL. Many coding problems adapted “wild” include kinds context usually available answering questions. purposes actually carrying initial evaluation, ’re specifically interested input target columns. Let’s print first entry full can get taste typical problem dataset: ’s suggested solution:","code":"glimpse(are) ## Rows: 29 ## Columns: 7 ## $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… ## $ input     <chr> \"This bar chart shows the count of different cuts o… ## $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … ## $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … ## $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… ## $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… ## $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t… cat(are$input[1]) ## This bar chart shows the count of different cuts of diamonds, ## and each bar is ## stacked and filled according to clarity: ##  ##  ## ``` ##  ## ggplot(data = diamonds) + ## geom_bar(mapping = aes(x = cut, fill = clarity)) ## ``` ##  ##  ## Could you change this code so that the proportion of diamonds ## with a given cut ## corresponds to the bar height and not the count? Each bar ## should still be ## filled according to clarity. cat(are$target[1]) ## Preferably: ##  ## ``` ## ggplot(data = diamonds) + ## geom_bar(aes(x = cut, y = after_stat(count) / ## sum(after_stat(count)), fill = clarity)) ## ``` ##  ## or: ##  ## ``` ## ggplot(data = diamonds) + ## geom_bar(mapping = aes(x = cut, y = ..prop.., group = clarity, ## fill = clarity)) ## ``` ##  ## or: ##  ## ``` ## ggplot(data = diamonds) + ## geom_bar(mapping = aes(x = cut, y = after_stat(count / ## sum(count)), group = clarity, fill = clarity)) ## ``` ##  ## The dot-dot notation (`..count..`) was deprecated in ggplot2 ## 3.4.0, but it ## still works and should receive full credit: ##  ## ``` ## ggplot(data = diamonds) + ## geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = ## clarity)) ## ``` ##  ## Simply setting `position = \"fill\"` will result in each bar ## having a height of 1 ## and is not correct."},{"path":"https://vitals.tidyverse.org/dev/articles/vitals.html","id":"creating-and-evaluating-a-task","dir":"Articles","previous_headings":"","what":"Creating and evaluating a task","title":"Getting started with vitals","text":"LLM evaluation vitals happens two main steps: Use Task$new() situate dataset, solver, scorer Task. Use Task$eval() evaluate solver, evaluate scorer, explore persistent log results interactive Inspect log viewer. evaluation, task contains information solving scoring steps. ’s model responded first question : task also contains score information scoring step. ’ve used model_graded_qa() scorer, uses another model evaluate quality solver’s solutions reference solutions target column. model_graded_qa() model-graded scorer provided package. step compares Claude’s solutions reference solutions target column, assigning score solution using another model. score either 1 0, though since ’ve set partial_credit = TRUE, model can also choose allot response .5. vitals use model generated final response model score solutions. Hold , though—’re using LLM generate responses questions, using LLM grade responses?  technique called “model grading” “LLM---judge.” Done correctly, model grading effective scalable solution scoring. said, ’s without faults. ’s grading model thought response:","code":"are_task <- Task$new(   dataset = are,   solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),   scorer = model_graded_qa(partial_credit = TRUE),   name = \"An R Eval\" )  are_task are_task$eval() cat(are_task$get_samples()$result[1]) ## I'll modify the code to show the proportion of diamonds with ## each cut instead of the count, while still stacking by ## clarity. ##  ## To achieve this, we need to: ## 1. Calculate the proportion of each cut from the total number ## of diamonds ## 2. Use `position = \"fill\"` which will normalize each bar to ## represent 100% (proportion = 1) ##  ## Here's the updated code: ##  ## ``` ## ggplot(data = diamonds) + ## geom_bar(mapping = aes(x = cut, fill = clarity), position = ## \"fill\") + ## labs(y = \"Proportion\") # Renaming the y-axis to make it clear ## it shows proportions ## ``` ##  ## This code will: ## - Create bars where the height of each bar is 1 (representing ## 100%) ## - Stack the different clarity groups within each cut ## - Show the proportional distribution of clarity within each ## cut category ## - The y-axis will now show the proportion rather than count ##  ## Each bar will have the same height (1.0 or 100%), allowing you ## to easily compare the proportional distribution of clarity ## across different cuts. cat(are_task$get_samples()$scorer_chat[[1]]$last_turn()@text) ## I need to evaluate whether the submission correctly implements ## the task of showing the proportion of diamonds with a given ## cut (rather than count) while maintaining clarity as the fill ## aesthetic. ##  ## The submission proposes using `position = \"fill\"`, which ## normalizes each bar to have a height of 1 (or 100%). However, ## this approach doesn't meet the criterion because: ##  ## 1. With `position = \"fill\"`, each bar represents the ## proportion of clarity categories WITHIN each cut, not the ## proportion of each cut relative to the total number of ## diamonds. ##  ## 2. The criterion explicitly states that \"Simply setting ## `position = 'fill'` will result in each bar having a height of ## 1 and is not correct.\" ##  ## 3. The correct implementations shown in the criterion all ## involve calculating the proportion of each cut relative to the ## total count of diamonds (using either `after_stat(count) / ## sum(after_stat(count))` or the deprecated `..count.. / ## sum(..count..)` notation). ##  ## The submission fails to implement the correct calculation to ## show the proportion of each cut relative to the total number ## of diamonds. It shows the proportional distribution of clarity ## within each cut, but that's not what was asked for. ##  ## GRADE: I"},{"path":"https://vitals.tidyverse.org/dev/articles/vitals.html","id":"analyzing-the-results","dir":"Articles","previous_headings":"","what":"Analyzing the results","title":"Getting started with vitals","text":"Especially first times run eval, ’ll want inspect (ha!) results closely. vitals package ships app, Inspect log viewer, allows drill solutions grading decisions model sample. first couple runs, ’ll likely find revisions can make grading guidance target align model responses intent.  hood, call task$eval(), results written .json file Inspect log viewer can read. Task object automatically launches viewer call task$eval() interactive session. can also view results time are_task$view(). can explore eval (package’s pkgdown site). cursory analysis, can start visualizing correct vs. partially correct vs. incorrect answers:  Claude answered fully correctly 18 29 samples, partially correctly 4 times., leads sorts questions: models cheaper Claude just well? even local model? models available better box? Claude better allow “reason” briefly answering? Claude better gave tools ’d allow peruse documentation /run R code answering? (See btw::btw_tools() ’re interested .) questions can explored evaluating Tasks different solvers scorers. example, compare Claude’s performance OpenAI’s GPT-4o, just need clone object run $eval() different solver chat: arguments solving scoring functions can passed directly $eval(), allowing quickly evaluating tasks across several parameterizations. Using data, can quickly juxtapose evaluation results:  difference performance just result noise, though? can supply scores ordinal regression model answer question. coefficient model == \"GPT-4o\" -0.944, indicating GPT-4o tends associated lower grades. 95% confidence interval coefficient contains zero, can conclude sufficient evidence reject null hypothesis difference GPT-4o Claude’s performance eval zero 0.05 significance level. evaluated model across multiple epochs, question ID become “nuisance parameter” mixed model, e.g. model structure ordinal::clmm(score ~ model + (1|id), ...). vignette demonstrated simplest possible evaluation based dataset. ’re interested carrying advanced evals, check vignettes package!","code":"are_task_data <- vitals_bind(are_task)  are_task_data ## # A tibble: 29 × 4 ##    task     id                          score metadata          ##    <chr>    <chr>                       <ord> <list>            ##  1 are_task after-stat-bar-heights      I     <tibble [1 × 10]> ##  2 are_task conditional-grouped-summary C     <tibble [1 × 10]> ##  3 are_task correlated-delays-reasoning P     <tibble [1 × 10]> ##  4 are_task curl-http-get               I     <tibble [1 × 10]> ##  5 are_task dropped-level-legend        I     <tibble [1 × 10]> ##  6 are_task filter-multiple-conditions  C     <tibble [1 × 10]> ##  7 are_task geocode-req-perform         C     <tibble [1 × 10]> ##  8 are_task group-by-summarize-message  C     <tibble [1 × 10]> ##  9 are_task grouped-filter-summarize    P     <tibble [1 × 10]> ## 10 are_task grouped-geom-line           C     <tibble [1 × 10]> ## # ℹ 19 more rows are_task_data |>   ggplot() +   aes(x = score) +   geom_bar() are_task_openai <- are_task$clone() are_task_openai$eval(solver_chat = chat_openai(model = \"gpt-4o\")) are_task_eval <-   vitals_bind(are_task, are_task_openai) |>   mutate(     task = if_else(task == \"are_task\", \"Claude\", \"GPT-4o\")   ) |>   rename(model = task)  are_task_eval |>   mutate(     score = factor(       case_when(         score == \"I\" ~ \"Incorrect\",         score == \"P\" ~ \"Partially correct\",         score == \"C\" ~ \"Correct\"       ),       levels = c(\"Incorrect\", \"Partially correct\", \"Correct\"),       ordered = TRUE     )   ) |>   ggplot(aes(y = model, fill = score)) +   geom_bar() +   scale_fill_brewer(breaks = rev, palette = \"RdYlGn\") library(ordinal) ##  ## Attaching package: 'ordinal' ## The following object is masked from 'package:dplyr': ##  ##     slice are_mod <- clm(score ~ model, data = are_task_eval)  are_mod ## formula: score ~ model ## data:    are_task_eval ##  ##  link  threshold nobs logLik AIC    niter max.grad cond.H  ##  logit flexible  58   -60.05 126.10 4(0)  3.53e-12 1.2e+01 ##  ## Coefficients: ## modelGPT-4o  ##     -0.9443  ##  ## Threshold coefficients: ##     I|P     P|C  ## -1.7248 -0.3048 confint(are_mod) ##                 2.5 %     97.5 % ## modelGPT-4o -1.967718 0.03831908"},{"path":"https://vitals.tidyverse.org/dev/articles/writing-evals.html","id":"how-to-write-good-evals","dir":"Articles","previous_headings":"","what":"How to write good evals","title":"Writing evals for your LLM product","text":"Writing eval vitals requires defining dataset, solver, scorer. ’ll speak elements individually.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/writing-evals.html","id":"datasets","dir":"Articles","previous_headings":"How to write good evals","what":"Datasets","title":"Writing evals for your LLM product","text":"vitals, datasets data frame columns, minimally, input target. “sample” row dataset. input defines questions inputted end users target defines target answer /grading guidance . sorts input prompts include, though? short, inputs natural. Rather “setting ” model exactly right context phrasing, “[]t’s important dataset… represents types interactions AI production” (Husain 2024a). system going answer set questions similar set already exists—support tickets, example—use actual tickets rather writing scratch. case, refrain correcting spelling errors, removing unneeded context, “sanitizing” providing system input; want distribution inputs resemble system encounter wild closely possible. existing resource input prompts pull , still try avoid sort unrealistic set-. ’ll specifically call multiple choice questions —multiple choice responses easy grade automatically, inputs provide system multiple choices select production system also access multiple choices (Press 2024). ’re writing questions, encourage read “Dimensions Structuring Dataset” section Husain (2024a), provides axes keep mind thinking generate data resembles system ultimately see: want define dimensions make sense use case. example, ones often use B2C applications: Features: Specific functionalities AI product. Scenarios: Situations problems AI may encounter needs handle. Personas: Representative user profiles distinct characteristics needs. part “” mechanics. probably don’t want paste bunch questions call tibble(), escape bunch quotes, etc. Instead, ’s ’ve written evals: Consider metadata want keep track along input target. want tag questions categories? Include source URL? isn’t stuff actually integrated eval, carried along throughout evaluation aid analysis line. Prompt model (Claude 4 Sonnet quite good ) make Shiny app help generate samples quickly (, hopefully, joyfully). app might provide free text box paste input another target, fields whatever metadata ’d like carry along, “Submit” button. click “Submit”, app write inputted values .yaml file directory “clear” app can add new sample. ’s sample prompt used help write eval dataset recently. , ’ve created sufficient number samples (30 good place start), read yaml files directory R, bind rows, ’ve got dataset. ’s sample script eval linked previous bullet.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/writing-evals.html","id":"solvers","dir":"Articles","previous_headings":"How to write good evals","what":"Solvers","title":"Writing evals for your LLM product","text":"solver AI product . product chat app app integrates chat feature, can supply ellmer Chat object powering chat directly solver_chat argument generate() ’re ready go. case, ensure prompt tools attached Chat object product.","code":""},{"path":"https://vitals.tidyverse.org/dev/articles/writing-evals.html","id":"scorers","dir":"Articles","previous_headings":"How to write good evals","what":"Scorers","title":"Writing evals for your LLM product","text":"Scorers take input, target, solver’s output, deliver judgment whether solver’s output satisfies grading guidance target closely enough. fundamental level, LLMs useful situations inputs outputs diverse considerably variable structure; result, determining correctness evals hard problem. implementing scorer, options Deterministic scoring: specific situations, ’s possible write code can check correctness without use LLM. example, solver chooses answer multiple choices, just use regular expression (scorers detect_match() friends). , scorer outputs code, check compiles/parses run code unit test. LLM---judge: situations, ’ll want pass input, solver’s result, grading guidance (target) model, ask model assign score. (vitals natively supports factors levels < P < C, standing Incorrect, (optional) Partially Correct, Correct.) Human grading: earliest (even develop scorer) latest (right product goes production) stages product development, spending lot time looking solver’s outputs . Understandably, many people bristle thought LLMs evaluating output; systems take careful refinement, “implemented well, LLM--Judge achieves decent correlation human judgments” (Yan et al. 2024). vitals safeguard many common LLM---judge missteps model_graded_qa(), recommendations implementing systems use cases: still looking lots data : “looking data, ’s likely find errors AI system. Instead plowing ahead building LLM judge, want fix obvious errors. Remember, whole point LLM judge help find errors, ’s totally fine find earlier” (Husain 2024a). Especially first implement LLM---judge, look reasoning output ensure aligns intuitions closely possible. Sometimes means fixing judge, sometimes means going back editing grading guidance samples . Start binary judgments: may tempting implement sort scoring rubric assigns numeric score output. Instead, start trying align scorer returns Pass/Fail intuitions whether output satisfactory. right instructions argument, might able get model_graded_qa() output trusted judgments reliably. One exception ’re measuring product’s ability follow set instructions. chores eval one example , scorer assigns binary judgment set scoring items, R code takes judgments sums create numeric score. Use strong models judge: production system, might interested driving costs reducing latency choosing smaller models solver. LLM---judge system, though, stick “flagship” models like Claude 4 Sonnet, GPT 4.1, Gemini 2.5 Pro, etc.; ’re much likely get results can begin trust larger models.","code":""},{"path":"https://vitals.tidyverse.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. Max Kuhn. Contributor. Hadley Wickham. Contributor. Mine Cetinkaya-Rundel. Contributor. . Copyright holder, funder.","code":""},{"path":"https://vitals.tidyverse.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). vitals: Large Language Model Evaluation. R package version 0.1.0.9000, https://github.com/tidyverse/vitals.","code":"@Manual{,   title = {vitals: Large Language Model Evaluation},   author = {Simon Couch},   year = {2025},   note = {R package version 0.1.0.9000},   url = {https://github.com/tidyverse/vitals}, }"},{"path":"https://vitals.tidyverse.org/dev/index.html","id":"vitals-","dir":"","previous_headings":"","what":"Large Language Model Evaluation","title":"Large Language Model Evaluation","text":"vitals framework large language model evaluation R. ’s specifically aimed ellmer users want measure effectiveness LLM products like custom chat apps querychat apps. can use : Measure whether changes prompts additions new tools improve performance LLM product Compare different models affect performance, cost, /latency LLM product Surface problematic behaviors LLM product package R port widely adopted Python framework Inspect. package doesn’t integrate Inspect directly, allows users interface Inspect log viewer provides -ramp transition Inspect need writing evaluation logs file format.","code":""},{"path":"https://vitals.tidyverse.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Large Language Model Evaluation","text":"Install vitals package CRAN : can install developmental version vitals using:","code":"install.packages(\"vitals\") pak::pak(\"tidyverse/vitals\")"},{"path":"https://vitals.tidyverse.org/dev/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Large Language Model Evaluation","text":"LLM evaluation vitals composed two main steps. First, create evaluation task Task$new() method. Tasks composed three main components: Datasets data frame , minimally, columns input target. input represents question problem, target gives target response. Solvers functions take input return value approximating target, likely wrapping ellmer chats. generate() simplest scorer vitals, just passes input chat’s $chat() method, returning result -. Scorers juxtapose solvers’ output target, evaluating well solver solved input. Evaluate task. $eval() run solver, run scorer, situate results persistent log file can explored interactively Inspect log viewer.  arguments solver scorer can passed $eval(), allowing straightforward parameterization tasks. example, wanted evaluate chat_openai() task rather chat_anthropic(), write: applied example, see “Getting started vitals” vignette vignette(\"vitals\", package = \"vitals\").","code":"library(vitals) library(ellmer) library(tibble) simple_addition <- tibble(   input = c(\"What's 2+2?\", \"What's 2+3?\", \"What's 2+4?\"),   target = c(\"4\", \"5\", \"6\") )  tsk <- Task$new(   dataset = simple_addition,    solver = generate(chat_anthropic(model = \"claude-sonnet-4-20250514\")),    scorer = model_graded_qa() ) tsk$eval() tsk_openai <- tsk$clone() tsk_openai$eval(solver_chat = chat_openai(model = \"gpt-4.1\"))"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":null,"dir":"Reference","previous_headings":"","what":"Creating and evaluating tasks — Task","title":"Creating and evaluating tasks — Task","text":"Evaluation Tasks provide flexible data structure evaluating LLM-based tools. Datasets contain set labelled samples. Datasets just tibble columns input target, input prompt target either literal value(s) grading guidance. Solvers evaluate input dataset produce final result. Scorers evaluate final output solvers. may use text comparisons (like detect_match()), model grading (like model_graded_qa()), custom schemes. usual flow LLM evaluation Tasks calls $new() $eval(). $eval() just calls $solve(), $score(), $measure(), $log(), $view() order. remaining methods generally recommended expert use.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Creating and evaluating tasks — Task","text":"dir directory evaluation logs written . Defaults vitals_log_dir(). metrics named vector metric values resulting $measure() (called inside $eval()). NULL metrics yet applied.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Creating and evaluating tasks — Task","text":"Task$new() Task$eval() Task$get_samples() Task$solve() Task$score() Task$measure() Task$log() Task$view() Task$set_solver() Task$set_scorer() Task$set_metrics() Task$get_cost() Task$clone()","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Creating and evaluating tasks — Task","text":"typical flow LLM evaluation vitals tends involve first calling method $eval() resulting object.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$new(   dataset,   solver,   scorer,   metrics = NULL,   epochs = NULL,   name = deparse(substitute(dataset)),   dir = vitals_log_dir() )"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"dataset tibble , minimally, columns input target. solver function takes vector inputs dataset's input column first argument determines values approximating dataset$target. return value must list following elements: result - character vector final responses, length dataset$input. solver_chat - list ellmer Chat objects used solve input, also length dataset$input. Additional output elements can included slot solver_metadata length dataset$input, logged solver_metadata. Additional arguments can passed solver via $solve(...) $eval(...). See definition generate() function outputs valid solver just passes inputs ellmer Chat objects' $chat() method parallel. scorer function evaluates well solver's return value approximates corresponding elements dataset$target. function take $get_samples() slot Task object return list following elements: score - vector scores length equal nrow(samples). Built-scorers return ordered factors levels < P (optionally) < C (standing \"Incorrect\", \"Partially Correct\", \"Correct\"). scorer returns output type, package automatically calculate metrics. Optionally: scorer_chat - scorer makes use ellmer, also include list ellmer Chat objects used score result, also length nrow(samples). scorer_metadata - intermediate results values like stored persistent log. also length equal nrow(samples). Scorers probably make use samples$input, samples$target, samples$result specifically. See model-based scoring examples. metrics named list functions take vector scores (task$get_samples()$score) output single numeric value. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new(). name name evaluation task. Defaults deparse(substitute(dataset)). dir Directory logs stored.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"new Task object.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-eval-","dir":"Reference","previous_headings":"","what":"Method eval()","title":"Creating and evaluating tasks — Task","text":"Evaluates task running solver, scorer, logging results, viewing (interactive). method works calling $solve(), $score(), $log(), $view() sequence. typical flow LLM evaluation vitals tends involve first calling $new() method resulting object.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$eval(..., epochs = NULL, view = interactive())"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed solver scorer functions. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new(). view Automatically open viewer evaluation (defaults TRUE interactive, FALSE otherwise).","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-get-samples-","dir":"Reference","previous_headings":"","what":"Method get_samples()","title":"Creating and evaluating tasks — Task","text":"task's samples represent evaluation data frame format. vitals_bind() row-binds output function called across several tasks.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$get_samples()"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"tibble representing evaluation. Based dataset, epochs may duplicate rows, solver scorer append columns data.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-solve-","dir":"Reference","previous_headings":"","what":"Method solve()","title":"Creating and evaluating tasks — Task","text":"Solve task running solver","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$solve(..., epochs = NULL)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed solver function. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new().","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-score-","dir":"Reference","previous_headings":"","what":"Method score()","title":"Creating and evaluating tasks — Task","text":"Score task running scorer applying metrics results.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$score(...)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed scorer function.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-measure-","dir":"Reference","previous_headings":"","what":"Method measure()","title":"Creating and evaluating tasks — Task","text":"Applies metrics scored Task.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$measure()"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-log-","dir":"Reference","previous_headings":"","what":"Method log()","title":"Creating and evaluating tasks — Task","text":"Log task directory. Note , VITALS_LOG_DIR envvar set, happen automatically $eval().","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$log(dir = vitals_log_dir())"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"dir directory write log .","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"path logged file, invisibly.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-view-","dir":"Reference","previous_headings":"","what":"Method view()","title":"Creating and evaluating tasks — Task","text":"View task results Inspect log viewer","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$view()"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-set-solver-","dir":"Reference","previous_headings":"","what":"Method set_solver()","title":"Creating and evaluating tasks — Task","text":"Set solver function","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_solver(solver)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"solver function takes vector inputs dataset's input column first argument determines values approximating dataset$target. return value must list following elements: result - character vector final responses, length dataset$input. solver_chat - list ellmer Chat objects used solve input, also length dataset$input. Additional output elements can included slot solver_metadata length dataset$input, logged solver_metadata. Additional arguments can passed solver via $solve(...) $eval(...). See definition generate() function outputs valid solver just passes inputs ellmer Chat objects' $chat() method parallel.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-set-scorer-","dir":"Reference","previous_headings":"","what":"Method set_scorer()","title":"Creating and evaluating tasks — Task","text":"Set scorer function","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_scorer(scorer)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"scorer function evaluates well solver's return value approximates corresponding elements dataset$target. function take $get_samples() slot Task object return list following elements: score - vector scores length equal nrow(samples). Built-scorers return ordered factors levels < P (optionally) < C (standing \"Incorrect\", \"Partially Correct\", \"Correct\"). scorer returns output type, package automatically calculate metrics. Optionally: scorer_chat - scorer makes use ellmer, also include list ellmer Chat objects used score result, also length nrow(samples). scorer_metadata - intermediate results values like stored persistent log. also length equal nrow(samples). Scorers probably make use samples$input, samples$target, samples$result specifically. See model-based scoring examples.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-set-metrics-","dir":"Reference","previous_headings":"","what":"Method set_metrics()","title":"Creating and evaluating tasks — Task","text":"Set metrics applied $measure() (thus $eval()).","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_metrics(metrics)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"metrics named list functions take vector scores (task$get_samples()$score) output single numeric value.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"Creating and evaluating tasks — Task","text":"cost eval wrapper around ellmer's $token_usage() function. function called beginning end call $solve() $score(); function returns cost inferred taking differences values $token_usage() time.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$get_cost()"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"tibble displaying cost solving scoring evaluation model, separately solver scorer.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Creating and evaluating tasks — Task","text":"objects class cloneable method.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$clone(deep = FALSE)"},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"deep Whether make deep clone.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/Task.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creating and evaluating tasks — Task","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    # evaluate the task (runs solver and scorer) and opens   # the results in the Inspect log viewer (if interactive)   tsk$eval()    # $eval() is shorthand for:   tsk$solve()   tsk$score()   tsk$measure()   tsk$log()   tsk$view()    # get the evaluation results as a data frame   tsk$get_samples()    # view the task directory with $view() or vitals_view()   vitals_view() } #> ℹ Solving #> ✔ Solving [2.4s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [3.2s] #>  #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576> #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576>"},{"path":"https://vitals.tidyverse.org/dev/reference/are.html","id":null,"dir":"Reference","previous_headings":"","what":"An R Eval — are","title":"An R Eval — are","text":"R Eval dataset challenging R coding problems. input question R code solved first-read experts , chance read documentation run code, fluent data scientists. Solutions target() enable fluent data scientist evaluate whether solution deserves full, partial, credit. Pass dataset Task$new() situate inside evaluation task.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/are.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An R Eval — are","text":"","code":"are"},{"path":"https://vitals.tidyverse.org/dev/reference/are.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An R Eval — are","text":"tibble 29 rows 7 columns: id Character. Unique identifier/title code problem. input Character. question answered. target Character. solution, often description notable features correct solution. domain Character. technical domain (e.g., Data Analysis, Programming, Authoring). task Character. Type task (e.g., Debugging, New feature, Translation.) source Character. URL source problem. NAs indicate problem written originally eval. knowledge List. Required knowledge/concepts solving problem.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/are.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"An R Eval — are","text":"Posit Community, GitHub issues, R4DS solutions, etc. row-level references, see source.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/are.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"An R Eval — are","text":"","code":"are #> # A tibble: 29 × 7 #>    id                        input target domain task  source knowledge #>    <chr>                     <chr> <chr>  <chr>  <chr> <chr>  <list>    #>  1 after-stat-bar-heights    \"Thi… \"Pref… Data … New … https… <chr [1]> #>  2 conditional-grouped-summ… \"I h… \"One … Data … New … https… <chr [1]> #>  3 correlated-delays-reason… \"Her… \"Nota… Data … New … NA     <chr [1]> #>  4 curl-http-get             \"I h… \"Ther… Progr… Debu… https… <chr [1]> #>  5 dropped-level-legend      \"I'd… \"Also… Data … New … https… <chr [1]> #>  6 filter-multiple-conditio… \"Her… \"This… Data … New … NA     <chr [1]> #>  7 geocode-req-perform       \"I a… \"You … Data … Debu… https… <chr [1]> #>  8 group-by-summarize-messa… \"Her… \"The … Data … New … NA     <chr [1]> #>  9 grouped-filter-summarize  \"Her… \"Ther… Data … New … NA     <chr [1]> #> 10 grouped-geom-line         \"Her… \"You … Data … New … NA     <chr [1]> #> # ℹ 19 more rows  dplyr::glimpse(are) #> Rows: 29 #> Columns: 7 #> $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… #> $ input     <chr> \"This bar chart shows the count of different cuts o… #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t…"},{"path":"https://vitals.tidyverse.org/dev/reference/generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a chat to a solver function — generate","title":"Convert a chat to a solver function — generate","text":"generate() simplest possible solver one might use vitals; just passes inputs supplied model returns raw responses. inputs evaluated parallel, sense multiple R sessions, sense multiple, asynchronous HTTP requests using ellmer::parallel_chat(). generate()'s output can passed directory solver argument Task's $new() method.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a chat to a solver function — generate","text":"","code":"generate(solver_chat = NULL)"},{"path":"https://vitals.tidyverse.org/dev/reference/generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a chat to a solver function — generate","text":"solver_chat ellmer chat object, ellmer::chat_anthropic().","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a chat to a solver function — generate","text":"output generate() another function. function takes vector inputs, well solver chat name solver_chat default supplied generate() . See documentation solver argument Task information return type.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a chat to a solver function — generate","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    # evaluate the task (runs solver and scorer) and opens   # the results in the Inspect log viewer (if interactive)   tsk$eval()    # $eval() is shorthand for:   tsk$solve()   tsk$score()   tsk$measure()   tsk$log()   tsk$view()    # get the evaluation results as a data frame   tsk$get_samples()    # view the task directory with $view() or vitals_view()   vitals_view() } #> ℹ Solving #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Solving #> ✔ Solving [2.6s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [5.8s] #>  #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576> #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576>"},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_detect.html","id":null,"dir":"Reference","previous_headings":"","what":"Scoring with string detection — scorer_detect","title":"Scoring with string detection — scorer_detect","text":"following functions use string pattern detection score model outputs. detect_includes(): Determine whether target sample appears anywhere inside model output. Can case sensitive insensitive (defaults latter). detect_match(): Determine whether target sample appears beginning end model output (defaults looking end). options ignoring case, white-space, punctuation (ignored default). detect_pattern(): Extract matches pattern model response determine whether matches also appear target. detect_answer(): Scorer model output precedes answers \"ANSWER: \". Can extract letters, words, remainder line. detect_exact(): Scorer normalize text answer target(s) perform exact matching comparison text. scorer return CORRECT answer exact match one targets.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_detect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scoring with string detection — scorer_detect","text":"","code":"detect_includes(case_sensitive = FALSE)  detect_match(   location = c(\"end\", \"begin\", \"end\", \"any\"),   case_sensitive = FALSE )  detect_pattern(pattern, case_sensitive = FALSE, all = FALSE)  detect_exact(case_sensitive = FALSE)  detect_answer(format = c(\"line\", \"word\", \"letter\"))"},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_detect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scoring with string detection — scorer_detect","text":"case_sensitive Logical, whether comparisons case sensitive. location look match: one \"begin\", \"end\", \"\", \"exact\". Defaults \"end\". pattern Regular expression pattern extract answer. Logical: multiple captures, whether must match. format extract \"ANSWER:\": \"letter\", \"word\", \"line\". Defaults \"line\".","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_detect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scoring with string detection — scorer_detect","text":"function scores model output based string matching. Pass returned value $eval(scorer). See documentation scorer argument Task information return type.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_detect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scoring with string detection — scorer_detect","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = detect_includes()   )    # evaluate the task (runs solver and scorer)   tsk$eval() } #> ℹ Solving #> ✔ Solving [1.1s] #>  #> ℹ Scoring #> ✔ Scoring [48ms] #>"},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Model-based scoring — scorer_model","title":"Model-based scoring — scorer_model","text":"Model-based scoring makes use model score output solver. model_graded_qa() scores well solver answers question/answer task. model_graded_fact() determines whether solver includes given fact response. two scorers quite similar implementation, use different default template evaluate correctness.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model-based scoring — scorer_model","text":"","code":"model_graded_qa(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   scorer_chat = NULL )  model_graded_fact(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   scorer_chat = NULL )"},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model-based scoring — scorer_model","text":"template Grading template use–glue() string take substitutions input, answer, criterion, instructions. instructions Grading instructions. grade_pattern regex pattern extract final grade judge model's response. partial_credit Whether allow partial credit. scorer_chat ellmer chat used grade model output, e.g. ellmer::chat_anthropic().","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model-based scoring — scorer_model","text":"function grade model responses according given instructions. See Task's scorer argument description returned function. functions model_graded_qa() model_graded_fact() output can passed directly $eval(). See documentation scorer argument Task information return type.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/dev/reference/scorer_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model-based scoring — scorer_model","text":"","code":"# Quality assurance ----------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- Task$new(     dataset = simple_addition,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    tsk$eval() } #> ℹ Solving #> ✔ Solving [1.6s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [4.1s] #>   # Factual response ------------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    r_history <- tibble(     input = c(       \"Who created the R programming language?\",       \"In what year was version 1.0 of R released?\"     ),     target = c(\"Ross Ihaka and Robert Gentleman.\", \"2000.\")   )    tsk <- Task$new(     dataset = r_history,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_fact()   )    tsk$eval() } #> ℹ Solving #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Solving #> ✔ Solving [4.2s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [3.8s] #>"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals-package.html","id":null,"dir":"Reference","previous_headings":"","what":"vitals: Large Language Model Evaluation — vitals-package","title":"vitals: Large Language Model Evaluation — vitals-package","text":"port 'Inspect', widely adopted 'Python' framework large language model evaluation. Specifically aimed 'ellmer' users want measure effectiveness large language model-based products, package supports prompt engineering, tool usage, multi-turn dialog, model graded evaluations.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/dev/reference/vitals-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"vitals: Large Language Model Evaluation — vitals-package","text":"Maintainer: Simon Couch simon.couch@posit.co (ORCID) contributors: Max Kuhn max@posit.co [contributor] Hadley Wickham hadley@posit.co (ORCID) [contributor] Mine Cetinkaya-Rundel mine@posit.co (ORCID) [contributor] Posit Software, PBC (03wc8by49) [copyright holder, funder]","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bind.html","id":null,"dir":"Reference","previous_headings":"","what":"Concatenate task samples for analysis — vitals_bind","title":"Concatenate task samples for analysis — vitals_bind","text":"Combine multiple Task objects single tibble comparison. function takes multiple (optionally named) Task objects row-binds $get_samples() together, adding task column identify source row. resulting tibble nests additional columns metadata column ready analysis.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concatenate task samples for analysis — vitals_bind","text":"","code":"vitals_bind(...)"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concatenate task samples for analysis — vitals_bind","text":"... Task objects combine, optionally named.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Concatenate task samples for analysis — vitals_bind","text":"tibble combined samples tasks, task column indicating source nested metadata column containing additional fields.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bind.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Concatenate task samples for analysis — vitals_bind","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk1 <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )   tsk1$eval()    tsk2 <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = detect_includes()   )   tsk2$eval()    combined <- vitals_bind(model_graded = tsk1, string_detection = tsk2) } #> ℹ Solving #> ✔ Solving [1.4s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [4.2s] #>  #> ℹ Solving #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Solving #> ✔ Solving [2.2s] #>  #> ℹ Scoring #> ✔ Scoring [44ms] #>"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare logs for deployment — vitals_bundle","title":"Prepare logs for deployment — vitals_bundle","text":"function creates standalone bundle Inspect viewer log files can deployed statically. copies UI viewer files, log files, generates necessary configuration files.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare logs for deployment — vitals_bundle","text":"","code":"vitals_bundle(log_dir = vitals_log_dir(), output_dir = NULL, overwrite = FALSE)"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare logs for deployment — vitals_bundle","text":"log_dir Path directory containing log files. Defaults vitals_log_dir(). output_dir Path directory bundled output placed. overwrite Whether overwrite existing output directory. Defaults FALSE.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare logs for deployment — vitals_bundle","text":"Invisibly returns output directory path. directory contains:   robots.txt prevents crawlers indexing viewer. said, many crawlers read robots.txt root directory package, file likely ignored folder root directory deployed page. assets/ bundled source viewer. logs/ log_dir well logs.json, manifest file directory.","code":"output_dir |-- index.html |-- robots.txt |-- assets     |--  .. |-- logs     |--  .."},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":"deployment","dir":"Reference","previous_headings":"","what":"Deployment","title":"Prepare logs for deployment — vitals_bundle","text":"function generates directory ready deployment static web server GitHub Pages, S3 buckets, Netlify. connection Posit Connect configured, can deploy directory log files following:","code":"tmp_dir <- withr::local_tempdir() vitals_bundle(output_dir = tmp_dir, overwrite = TRUE) rsconnect::deployApp(tmp_dir)"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_bundle.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare logs for deployment — vitals_bundle","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    tsk$eval()    output_dir <- tempdir()   vitals_bundle(output_dir = output_dir, overwrite = TRUE) } #> ℹ Solving #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Solving #> ✔ Solving [11.7s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [13.1s] #>  #> ✔ Bundle /tmp/RtmpwVNliw created!"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_log_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"The log directory — vitals_log_dir","title":"The log directory — vitals_log_dir","text":"vitals supports VITALS_LOG_DIR environment variable, sets default directory write logs Task's $eval() $log() methods.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_log_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The log directory — vitals_log_dir","text":"","code":"vitals_log_dir()  vitals_log_dir_set(dir)"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_log_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The log directory — vitals_log_dir","text":"dir directory configure environment variable VITALS_LOG_DIR .","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_log_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The log directory — vitals_log_dir","text":"vitals_log_dir() vitals_log_dir_set() return current value environment variable VITALS_LOG_DIR. vitals_log_dir_set() additionally sets new value. set variable every new R session, might consider adding .Rprofile, perhaps usethis::edit_r_profile().","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_log_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The log directory — vitals_log_dir","text":"","code":"vitals_log_dir() #> [1] \"$RUNNER_TEMP\"  dir <- tempdir()  vitals_log_dir_set(dir)  vitals_log_dir() #> [1] \"/tmp/RtmpwVNliw\""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_view.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactively view local evaluation logs — vitals_view","title":"Interactively view local evaluation logs — vitals_view","text":"vitals bundles Inspect log viewer, interactive app exploring evaluation logs. Supply path directory tasks written json. individual Task objects, use $view() method instead.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_view.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactively view local evaluation logs — vitals_view","text":"","code":"vitals_view(dir = vitals_log_dir(), host = \"127.0.0.1\", port = 7576)"},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_view.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactively view local evaluation logs — vitals_view","text":"dir Path directory containing task eval logs. host Host serve . Defaults \"127.0.0.1\". port Port serve . Defaults 7576, one greater Python implementation.","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_view.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactively view local evaluation logs — vitals_view","text":"server object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/dev/reference/vitals_view.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactively view local evaluation logs — vitals_view","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   # set the log directory to a temporary directory   withr::local_envvar(VITALS_LOG_DIR = withr::local_tempdir())    library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    # evaluate the task (runs solver and scorer) and opens   # the results in the Inspect log viewer (if interactive)   tsk$eval()    # $eval() is shorthand for:   tsk$solve()   tsk$score()   tsk$measure()   tsk$log()   tsk$view()    # get the evaluation results as a data frame   tsk$get_samples()    # view the task directory with $view() or vitals_view()   vitals_view() } #> ℹ Solving #> ✔ Solving [1s] #>  #> ℹ Scoring #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ℹ Scoring #> ✔ Scoring [12.7s] #>  #> [working] (0 + 0) -> 1 -> 1 | ■■■■■■■■■■■■■■■■                  50% #> [working] (0 + 0) -> 0 -> 2 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576> #> ✔ Inspect Viewer running at: <http://127.0.0.1:7576>"},{"path":"https://vitals.tidyverse.org/dev/news/index.html","id":"vitals-development-version","dir":"Changelog","previous_headings":"","what":"vitals (development version)","title":"vitals (development version)","text":"package now set envvar IN_VITALS_EVAL \"true\" solving scoring. Numeric task targets longer introduce errors log viewer.","code":""},{"path":"https://vitals.tidyverse.org/dev/news/index.html","id":"vitals-010","dir":"Changelog","previous_headings":"","what":"vitals 0.1.0","title":"vitals 0.1.0","text":"CRAN release: 2025-06-24 Initial CRAN submission.","code":""}]
