% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/task.R
\name{Task}
\alias{Task}
\title{Creating and evaluating tasks with R6}
\description{
Evaluation \code{Task}s provide a flexible data structure for evaluating LLM-based
tools.
\enumerate{
\item \strong{Datasets} contain a set of labelled samples. Datasets are just a
tibble with columns \code{input} and \code{target}, where \code{input} is a prompt
and \code{target} is either literal value(s) or grading guidance.
\item \strong{Solvers} evaluate the \code{input} in the dataset and produce a final result.
The simplest solver is just an ellmer chat (e.g. \code{\link[ellmer:chat_claude]{ellmer::chat_claude()}}).
\item \strong{Scorers} evaluate the final output of solvers. They may use text
comparisons (like \code{\link[=detect_match]{detect_match()}}), model grading (like
\code{\link[=model_graded_qa]{model_graded_qa()}}), or other custom schemes.
}
}
\examples{
if (!identical(Sys.getenv("ANTHROPIC_API_KEY"), "")) {
  library(ellmer)
  library(tibble)

  simple_addition <- tibble(
    input = c("What's 2+2?", "What's 2+3?"),
    target = c("4", "5")
  )

  # create a new Task
  tsk <- Task$new(
    dataset = simple_addition, 
    solver = generate(chat = chat_claude()), 
    scorer = model_graded_qa()
  )

  # evaluate the task (runs solver and scorer) and opens
  # the results in the Inspect log viewer (if interactive)
  tsk$eval()
}

}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-Task-new}{\code{Task$new()}}
\item \href{#method-Task-eval}{\code{Task$eval()}}
\item \href{#method-Task-view}{\code{Task$view()}}
\item \href{#method-Task-log}{\code{Task$log()}}
\item \href{#method-Task-data}{\code{Task$data()}}
\item \href{#method-Task-clone}{\code{Task$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-new"></a>}}
\if{latex}{\out{\hypertarget{method-Task-new}{}}}
\subsection{Method \code{new()}}{
Create a new Task object
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$new(
  dataset,
  solver,
  scorer,
  name = deparse(substitute(dataset)),
  dir = inspect_log_dir()
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{dataset}}{A tibble with, minimally, columns \code{input} and \code{target}.}

\item{\code{solver}}{A function that takes the vector \code{dataset$input} as its first
argument and determines a value approximating \code{dataset$target}.
Its return value should be a list with elements \code{outputs} (a vector of the
final responses, the same length as \code{dataset$input}) and \code{solvers}
(the list of ellmer chats used to solve the inputs, also the same length
as \code{dataset$input}). Or, just supply an ellmer chat
(e.g. \code{\link[ellmer:chat_claude]{ellmer::chat_claude()}}) and rinspect will take care of the details.}

\item{\code{scorer}}{A function that evaluates how well the solver's return value
approximates the corresponding elements of \code{dataset$target}. See
\link[=scorer_model]{model-based scoring} for examples.}

\item{\code{name}}{A name for the evaluation task. Defaults to
\code{deparse(substitute(dataset))}.}

\item{\code{dir}}{Directory where logs should be stored.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-eval"></a>}}
\if{latex}{\out{\hypertarget{method-Task-eval}{}}}
\subsection{Method \code{eval()}}{
Evaluate the task by running the solver and scorer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$eval(..., epochs = 1L, view = interactive())}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments passed to the solver and scorer functions.}

\item{\code{epochs}}{The number of times to repeat each sample. Evaluate each sample
multiple times to measure variation. Optional, defaults to \code{1L}.}

\item{\code{view}}{Automatically open the viewer after evaluation (defaults to
TRUE if interactive, FALSE otherwise).}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
The Task object (invisibly)
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-view"></a>}}
\if{latex}{\out{\hypertarget{method-Task-view}{}}}
\subsection{Method \code{view()}}{
View the task results in the Inspect log viewer
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$view()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
The Task object (invisibly)
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-log"></a>}}
\if{latex}{\out{\hypertarget{method-Task-log}{}}}
\subsection{Method \code{log()}}{
Log the task to a directory.

Note that, if an \code{INSPECT_LOG_DIR} envvar is set, this will happen
automatically in \verb{$eval()}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$log(dir = inspect_log_dir())}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{dir}}{The directory to write the log to.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
The path to the logged file, invisibly.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-data"></a>}}
\if{latex}{\out{\hypertarget{method-Task-data}{}}}
\subsection{Method \code{data()}}{
Get the internal task tibble
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$data()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A tibble with the task data
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Task-clone"></a>}}
\if{latex}{\out{\hypertarget{method-Task-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Task$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
