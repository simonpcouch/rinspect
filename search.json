[{"path":[]},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://vitals.tidyverse.org/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://vitals.tidyverse.org/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 vitals authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://vitals.tidyverse.org/articles/solvers.html","id":"an-r-eval-dataset","dir":"Articles","previous_headings":"","what":"An R eval dataset","title":"Custom solvers and tool calling","text":"docs: R Eval dataset challenging R coding problems. input question R code solved first-read human experts , chance read documentation run code, fluent data scientists. Solutions target enable fluent data scientist evaluate whether solution deserves full, partial, credit. high level: id: unique identifier problem. input: question answered. target: solution, often description notable features correct solution. domain, task, knowledge pieces metadata describing kind R coding challenge. source: problem came , URL. Many coding problems adapted “wild” include kinds context usually available answering questions. introductory vignette, just provided inputs -ellmer chat’s $chat() method (via built-generate() solver). vignette, ’ll compare results approach resulting first giving models chance read relevant R package documentation.","code":"glimpse(are) #> Rows: 26 #> Columns: 7 #> $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… #> $ input     <chr> \"This bar chart shows the count of different cuts o… #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t…"},{"path":"https://vitals.tidyverse.org/articles/solvers.html","id":"custom-solvers","dir":"Articles","previous_headings":"","what":"Custom solvers","title":"Custom solvers and tool calling","text":"help us better understand solvers work, lets look implementation generate(). generate() function factory, meaning function outputs function. , documentation, ’ve mostly written “generate() just calls $chat(),” couple extra tricks generate()’s sleeves: chat first cloned $clone() original chat object isn’t modified vitals calls . Rather $chat(), generate() calls $chat_parallel(). generate() takes inputs–e.g. whole input column–rather single input. $chat_parallel() send requests parallel (usual R sense multiple R processes, sense asychronous HTTP requests), greatly reducing amount time takes run evaluations. function generate outputs takes list inputs outputs list two elements: result final result solver. case, purrr::map_chr(res, function(c) c$last_turn()@text) just grabs text last assistant run returns . result vector length equal inputs—able tacked onto task$get_samples() another column. solver_chat list chats led result. may seem redundant result, important supply vitals package can log happened solving process display log viewer. work slightly modified version generate() equips models ability peruse R package documentation answering. using btw package, available GitHub https://posit-dev.github.io/btw/. Among things, btw provides function btw_register_tools() registers ellmer chat tools explore documentation. btw_register_tools() modifies chat object place, can use chat’s $chat() method interact like . example, registering btw Claude Sonnet 3.7 chat object, can ask package. time writing, vitals CRAN, Claude Sonnet 3.7 completed training several months started working . Nonetheless, model able tell custom solvers work: ’ll help find information custom solvers vitals package. Let first check vitals package installed explore documentation. models lets us know ’s reading bunch documentation.<> create custom solver, need : Takes vector inputs first argument result: vector final responses (length input) solver_chat: list ellmer chat objects used (length input) Preprocess inputs sending LLM Post-process model responses Implement special handling like multi-turn conversations Incorporate tools external knowledge Add retry mechanisms fallbacks Yada yada yada. Pretty spot-. capability help models better answer questions R? built questions base R popular packages like dplyr shiny, information presumably baked weights. implementation custom solver looks almost exactly like generate()s implementation, except add call btw_register_tools() calling $chat_parallel(). solver ready situate Task get solving!","code":"sonnet_3_7 <- chat_anthropic(model = \"claude-3-7-sonnet-latest\") generate(solver_chat = sonnet_3_7$clone()) #> function (inputs, ..., solver_chat = chat)  #> { #>     check_inherits(solver_chat, \"Chat\") #>     ch <- solver_chat$clone() #>     res <- ch$chat_parallel(as.list(inputs), ...) #>     list(result = purrr::map_chr(res, function(c) c$last_turn()@text),  #>         solver_chat = res) #> } #> <bytecode: 0x565447424200> #> <environment: 0x5654474231d0> ch <- sonnet_3_7$clone() btw_register_tools(ch, tools = \"docs\") ✔ Registered tool btw_tool_docs_package_help_topics ✔ Registered tool btw_tool_docs_help_page ✔ Registered tool btw_tool_docs_available_vignettes ✔ Registered tool btw_tool_docs_vignette ch$chat(\"How do custom solvers work in vitals?\") btw_solver <- function(inputs, ..., solver_chat) {   ch <- solver_chat$clone()   btw_register_tools(ch, tools = \"docs\")   res <- ch$chat_parallel(as.list(inputs))   list(     result = purrr::map_chr(res, function(c) c$last_turn()@text),      solver_chat = res   ) }"},{"path":"https://vitals.tidyverse.org/articles/solvers.html","id":"running-the-solver","dir":"Articles","previous_headings":"","what":"Running the solver","title":"Custom solvers and tool calling","text":"Situate custom solver dataset way built-one: introductory vignette, supply dataset built-scorer model_graded_qa(). , use $eval() evaluate custom solver, evaluate scorer, explore persistent log results interactive Inspect log viewer. arguments solving scoring functions can passed directly $eval(), allowing quickly evaluating tasks across several parameterizations. ’ll just use Claude Sonnet 3.7 now. Claude answered fully correctly 0 26 samples, partially correctly 7 times. Using metadata, can also determine often model looked least one help-page supplying answer: equipped ability peruse documentation, model almost always made use . see kinds docs decided look , click samples log viewer eval:  perusing documentation actually help model write better R code, though? order quantify “better,” need also run evaluation without model able access documentation. Using code introductory vignette: (Note also used are_btw$set_solver() didn’t want recreate task.) difference performance just result noise, though? can supply scores ordinal regression model answer question. coefficient solver == \"btw\" -2.333, indicating allowing model peruse documentation tends associated lower grades. 95% confidence interval coefficient contains zero, can conclude sufficient evidence reject null hypothesis difference GPT-4o Claude’s performance eval zero 0.05 significance level. evaluated model across multiple epochs, question ID become “nuisance parameter” mixed model, e.g. model structure ordinal::clmm(score ~ model + (1|id), ...). article demonstrated using custom solver modifying built-generate() allow models make tool calls peruse package documentation. vitals, tool calling “just works”; calls tool registered ellmer automatically incorporated evaluation logs.","code":"are_btw <- Task$new(   dataset = are,   solver = btw_solver,   scorer = model_graded_qa(partial_credit = TRUE, scorer_chat = sonnet_3_7$clone()),   name = \"An R Eval\" )  are_btw are_btw$eval(solver_chat = sonnet_3_7$clone()) are_btw_data <- vitals_bind(are_btw)  are_btw_data #> # A tibble: 26 × 4 #>    task    id                          score metadata          #>    <chr>   <chr>                       <ord> <list>            #>  1 are_btw after-stat-bar-heights      I     <tibble [1 × 10]> #>  2 are_btw conditional-grouped-summary P     <tibble [1 × 10]> #>  3 are_btw correlated-delays-reasoning I     <tibble [1 × 10]> #>  4 are_btw curl-http-get               I     <tibble [1 × 10]> #>  5 are_btw dropped-level-legend        I     <tibble [1 × 10]> #>  6 are_btw geocode-req-perform         I     <tibble [1 × 10]> #>  7 are_btw ggplot-breaks-feature       I     <tibble [1 × 10]> #>  8 are_btw grouped-filter-summarize    P     <tibble [1 × 10]> #>  9 are_btw grouped-mutate              I     <tibble [1 × 10]> #> 10 are_btw implement-nse-arg           P     <tibble [1 × 10]> #> # ℹ 16 more rows chat_called_a_tool <- function(chat) {   turns <- chat[[1]]$get_turns()   any(purrr::map_lgl(turns, turn_called_a_tool)) }  turn_called_a_tool <- function(turn) {   any(purrr::map_lgl(turn@contents, inherits, \"ellmer::ContentToolRequest\")) }  are_btw_data |>   tidyr::hoist(metadata, \"solver_chat\") |>   mutate(called_a_tool = purrr::map_lgl(solver_chat, chat_called_a_tool)) |>   pull(called_a_tool) |>    table() #>  #> FALSE  TRUE  #>     3    23 are_basic <- Task$new(   dataset = are,   solver = generate(solver_chat = sonnet_3_7$clone()),   scorer = model_graded_qa(partial_credit = TRUE, scorer_chat = sonnet_3_7$clone()),   name = \"An R Eval\" )  are_basic$eval() are_eval_data <-    # the argument names will become the entries in `task`   vitals_bind(btw = are_btw, basic = are_basic) |>   # and, in this case, different tasks correspond to different solvers   rename(solver = task)  are_eval_data #> # A tibble: 52 × 4 #>    solver id                          score metadata          #>    <chr>  <chr>                       <ord> <list>            #>  1 btw    after-stat-bar-heights      I     <tibble [1 × 10]> #>  2 btw    conditional-grouped-summary P     <tibble [1 × 10]> #>  3 btw    correlated-delays-reasoning I     <tibble [1 × 10]> #>  4 btw    curl-http-get               I     <tibble [1 × 10]> #>  5 btw    dropped-level-legend        I     <tibble [1 × 10]> #>  6 btw    geocode-req-perform         I     <tibble [1 × 10]> #>  7 btw    ggplot-breaks-feature       I     <tibble [1 × 10]> #>  8 btw    grouped-filter-summarize    P     <tibble [1 × 10]> #>  9 btw    grouped-mutate              I     <tibble [1 × 10]> #> 10 btw    implement-nse-arg           P     <tibble [1 × 10]> #> # ℹ 42 more rows library(ordinal) #>  #> Attaching package: 'ordinal' #> The following object is masked from 'package:dplyr': #>  #>     slice  are_mod <- clm(score ~ solver, data = are_eval_data)  are_mod #> formula: score ~ solver #> data:    are_eval_data #>  #>  link  threshold nobs logLik AIC   niter max.grad cond.H  #>  logit flexible  52   -45.46 96.92 4(0)  1.25e-07 9.0e+00 #>  #> Coefficients: #> solverbtw  #>    -2.333  #>  #> Threshold coefficients: #>     I|P     P|C  #> -1.2549  0.4655 confint(are_mod) #>               2.5 %    97.5 % #> solverbtw -3.617944 -1.173259"},{"path":"https://vitals.tidyverse.org/articles/vitals.html","id":"an-r-eval-dataset","dir":"Articles","previous_headings":"","what":"An R eval dataset","title":"Getting started with vitals","text":"docs: R Eval dataset challenging R coding problems. input question R code solved first-read human experts , chance read documentation run code, fluent data scientists. Solutions target enable fluent data scientist evaluate whether solution deserves full, partial, credit. high level: id: unique identifier problem. input: question answered. target: solution, often description notable features correct solution. domain, task, knowledge pieces metadata describing kind R coding challenge. source: problem came , URL. Many coding problems adapted “wild” include kinds context usually available answering questions. purposes actually carrying initial evaluation, ’re specifically interested input target columns. Let’s print first entry full can get taste typical problem dataset: ’s suggested solution:","code":"glimpse(are) #> Rows: 26 #> Columns: 7 #> $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… #> $ input     <chr> \"This bar chart shows the count of different cuts o… #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t… cat(are$input[1]) #> This bar chart shows the count of different cuts of diamonds, #> and each bar is stacked and filled according to clarity: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(mapping = aes(x = cut, fill = clarity)) #> ``` #>  #> Could you change this code so that the proportion of diamonds #> with a given cut corresponds to the bar height and not the #> count? Each bar should still be filled according to clarity. cat(are$target[1]) #> Preferably: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(aes(x = cut, y = after_stat(count) / #> sum(after_stat(count)), fill = clarity)) #> ``` #>  #> The dot-dot notation (`..count..`) was deprecated in ggplot2 #> 3.4.0, but it still works: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = #> clarity)) #> ``` #>  #> Simply setting `position = \"fill\" will result in each bar #> having a height of 1 and is not correct."},{"path":"https://vitals.tidyverse.org/articles/vitals.html","id":"creating-and-evaluating-a-task","dir":"Articles","previous_headings":"","what":"Creating and evaluating a task","title":"Getting started with vitals","text":"LLM evaluation vitals happens two main steps: Use Task$new() situate dataset, solver, scorer Task. Use Task$eval() evaluate solver, evaluate scorer, explore persistent log results interactive Inspect log viewer. evaluation, task contains information solving scoring steps. ’s model responded first question : task also contains score information scoring step. ’ve used model_graded_qa() scorer, uses another model evaluate quality solver’s solutions reference solutions target column. model_graded_qa() model-graded scorer provided package. step compares Claude’s solutions reference solutions target column, assigning score solution using another model. score either 1 0, though since ’ve set partial_credit = TRUE, model can also choose allot response .5. vitals use model generated final response model score solutions. Hold , though—’re using LLM generate responses questions, using LLM grade responses?  technique called “model grading” “LLM---judge.” Done correctly, model grading effective scalable solution scoring. said, ’s without faults. ’s grading model thought response:","code":"are_task <- Task$new(   dataset = are,   solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),   scorer = model_graded_qa(partial_credit = TRUE),   name = \"An R Eval\" )  are_task are_task$eval() cat(are_task$get_samples()$result[1]) #> # Modifying the Bar Chart to Show Proportions by Cut #>  #> To change the bar chart so that it shows proportions rather #> than counts, I'll modify the code to calculate the proportion #> of diamonds with each cut relative to the total number of #> diamonds. #>  #> Here's the updated code: #>  #> ```r #> ggplot(data = diamonds) + #> geom_bar(mapping = aes(x = cut, fill = clarity), position = #> \"fill\") + #> labs(y = \"Proportion\") #> ``` #>  #> This change uses `position = \"fill\"` in the `geom_bar()` #> function, which: #>  #> 1. Stacks the bars as before, but then scales them to have the #> same height #> 2. Makes each bar represent 100% of the diamonds with that cut #> 3. Shows the proportion of each clarity category within each #> cut #>  #> The resulting y-axis will show proportions from 0 to 1 instead #> of counts, and I've added a y-axis label to clarify this. Each #> bar will still be filled according to clarity as in the #> original chart. cat(are_task$get_samples()$scorer_chat[[1]]$last_turn()@text) #> I need to evaluate whether the submitted answer meets the #> criterion, which specifies how to properly display the #> proportion of diamonds with a given cut as the bar height. #>  #> The submitted answer suggests using: #> ```r #> ggplot(data = diamonds) + #> geom_bar(mapping = aes(x = cut, fill = clarity), position = #> \"fill\") + #> labs(y = \"Proportion\") #> ``` #>  #> However, the criterion clearly states that using `position = #> \"fill\"` is not correct because it will result in each bar #> having a height of 1. This approach shows the proportion of #> clarity within each cut, but doesn't show the proportion of #> each cut relative to the total dataset. #>  #> The criterion specifies that the correct approach is to use: #> ```r #> ggplot(data = diamonds) + #> geom_bar(aes(x = cut, y = after_stat(count) / #> sum(after_stat(count)), fill = clarity)) #> ``` #>  #> The submission does not use this approach or anything #> equivalent. Instead, it uses the `position = \"fill\"` method #> which is explicitly stated as incorrect in the criterion. #>  #> GRADE: I"},{"path":"https://vitals.tidyverse.org/articles/vitals.html","id":"analyzing-the-results","dir":"Articles","previous_headings":"","what":"Analyzing the results","title":"Getting started with vitals","text":"Especially first times run eval, ’ll want inspect (ha!) results closely. vitals package ships app, Inspect log viewer, allows drill solutions grading decisions model sample. first couple runs, ’ll likely find revisions can make grading guidance target align model responses intent.  hood, call task$eval(), results written .json file Inspect log viewer can read. Task object automatically launches viewer call task$eval() interactive session. can also view results time are_task$view(). can explore eval (package’s pkgdown site). cursory analysis, can start visualizing correct vs. partially correct vs. incorrect answers:  Claude answered fully correctly 13 26 samples, partially correctly 5 times., leads sorts questions: models cheaper Claude just well? even local model? models available better box? Claude better allow “reason” briefly answering? Claude better gave tools ’d allow peruse documentation /run R code answering? (See btw::btw_register_tools() ’re interested .) questions can explored evaluating Tasks different solvers scorers. example, compare Claude’s performance OpenAI’s GPT-4o, just need clone object run $eval() different solver chat: arguments solving scoring functions can passed directly $eval(), allowing quickly evaluating tasks across several parameterizations. Using data, can quickly juxtapose evaluation results:  difference performance just result noise, though? can supply scores ordinal regression model answer question. coefficient model == \"GPT-4o\" -0.55, indicating GPT-4o tends associated lower grades. 95% confidence interval coefficient contains zero, can conclude sufficient evidence reject null hypothesis difference GPT-4o Claude’s performance eval zero 0.05 significance level. evaluated model across multiple epochs, question ID become “nuisance parameter” mixed model, e.g. model structure ordinal::clmm(score ~ model + (1|id), ...). vignette demonstrated simplest possible evaluation based dataset. ’re interested carrying advanced evals, check vignettes package!","code":"are_task_data <- vitals_bind(are_task)  are_task_data #> # A tibble: 26 × 4 #>    task     id                          score metadata          #>    <chr>    <chr>                       <ord> <list>            #>  1 are_task after-stat-bar-heights      I     <tibble [1 × 10]> #>  2 are_task conditional-grouped-summary C     <tibble [1 × 10]> #>  3 are_task correlated-delays-reasoning I     <tibble [1 × 10]> #>  4 are_task curl-http-get               C     <tibble [1 × 10]> #>  5 are_task dropped-level-legend        I     <tibble [1 × 10]> #>  6 are_task geocode-req-perform         C     <tibble [1 × 10]> #>  7 are_task ggplot-breaks-feature       I     <tibble [1 × 10]> #>  8 are_task grouped-filter-summarize    P     <tibble [1 × 10]> #>  9 are_task grouped-mutate              C     <tibble [1 × 10]> #> 10 are_task implement-nse-arg           P     <tibble [1 × 10]> #> # ℹ 16 more rows  are_task_data |>   ggplot() +   aes(x = score) +   geom_bar() are_task_openai <- are_task$clone() are_task_openai$eval(solver_chat = chat_openai(model = \"gpt-4o\")) are_task_eval <-   vitals_bind(are_task, are_task_openai) |>   mutate(     task = if_else(task == \"are_task\", \"Claude\", \"GPT-4o\")   ) |>   rename(model = task)  are_task_eval |>   mutate(     score = factor(       case_when(         score == \"I\" ~ \"Incorrect\",         score == \"P\" ~ \"Partially correct\",         score == \"C\" ~ \"Correct\"       ),       levels = c(\"Incorrect\", \"Partially correct\", \"Correct\"),       ordered = TRUE     )   ) |>   ggplot(aes(y = model, fill = score)) +   geom_bar() +   scale_fill_brewer(breaks = rev, palette = \"RdYlGn\") library(ordinal) #>  #> Attaching package: 'ordinal' #> The following object is masked from 'package:dplyr': #>  #>     slice  are_mod <- clm(score ~ model, data = are_task_eval)  are_mod #> formula: score ~ model #> data:    are_task_eval #>  #>  link  threshold nobs logLik AIC    niter max.grad cond.H  #>  logit flexible  52   -56.26 118.51 3(0)  1.54e-07 1.2e+01 #>  #> Coefficients: #> modelGPT-4o  #>     -0.5504  #>  #> Threshold coefficients: #>     I|P     P|C  #> -1.1235  0.1806 confint(are_mod) #>                 2.5 %    97.5 % #> modelGPT-4o -1.581626 0.4585473"},{"path":"https://vitals.tidyverse.org/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. . Copyright holder, funder.","code":""},{"path":"https://vitals.tidyverse.org/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). vitals: Large Language Model Evaluation R. R package version 0.0.0.9002, https://github.com/tidyverse/vitals.","code":"@Manual{,   title = {vitals: Large Language Model Evaluation for R},   author = {Simon Couch},   year = {2025},   note = {R package version 0.0.0.9002},   url = {https://github.com/tidyverse/vitals}, }"},{"path":"https://vitals.tidyverse.org/index.html","id":"vitals-","dir":"","previous_headings":"","what":"Large Language Model Evaluation for R","title":"Large Language Model Evaluation for R","text":"vitals framework large language model evaluation R. ’s specifically aimed ellmer users want measure effectiveness LLM-based apps. package R port widely adopted Python framework Inspect. package doesn’t integrate Inspect directly, allows users interface Inspect log viewer provides -ramp transition Inspect need writing evaluation logs file format. Important 🚧 construction! 🚧 vitals highly experimental much documentation aspirational.","code":""},{"path":"https://vitals.tidyverse.org/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Large Language Model Evaluation for R","text":"can install developmental version vitals using:","code":"pak::pak(\"tidyverse/vitals\")"},{"path":"https://vitals.tidyverse.org/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Large Language Model Evaluation for R","text":"LLM evaluation vitals composed two main steps. First, create evaluation task Task$new() method. Tasks composed three main components: Datasets data frame , minimally, columns input target. input represents question problem, target gives target response. Solvers functions take input return value approximating target, likely wrapping ellmer chats. generate() simplest scorer vitals, just passes input chat’s $chat() method, returning result -. Scorers juxtapose solvers’ output target, evaluating well solver solved input. Evaluate task. $eval() run solver, run scorer, situate results persistent log file can explored interactively Inspect log viewer.  arguments solver scorer can passed $eval(), allowing straightforward parameterization tasks. example, wanted evaluate chat_openai() task rather chat_anthropic(), write: applied example, see “Getting started vitals” vignette vignette(\"vitals\", package = \"vitals\").","code":"library(vitals) library(ellmer) library(tibble) simple_addition <- tibble(   input = c(\"What's 2+2?\", \"What's 2+3?\", \"What's 2+4?\"),   target = c(\"4\", \"5\", \"6\") )  tsk <- Task$new(   dataset = simple_addition,    solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),    scorer = model_graded_qa() ) tsk$eval() tsk_openai <- tsk$clone() tsk_openai$eval(solver_chat = chat_openai(model = \"gpt-4o\"))"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":null,"dir":"Reference","previous_headings":"","what":"Creating and evaluating tasks — Task","title":"Creating and evaluating tasks — Task","text":"Evaluation Tasks provide flexible data structure evaluating LLM-based tools. Datasets contain set labelled samples. Datasets just tibble columns input target, input prompt target either literal value(s) grading guidance. Solvers evaluate input dataset produce final result. Scorers evaluate final output solvers. may use text comparisons (like detect_match()), model grading (like model_graded_qa()), custom schemes. usual flow LLM evaluation Tasks calls $new() $eval(). $eval() just calls $solve(), $score(), $measure(), $log(), $view() order. remaining methods generally recommended expert use.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Creating and evaluating tasks — Task","text":"dir directory evaluation logs written . Defaults vitals_log_dir(). metrics named vector metric values resulting $measure() (called inside $eval()). NULL metrics yet applied.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Creating and evaluating tasks — Task","text":"Task$new() Task$eval() Task$get_samples() Task$solve() Task$score() Task$measure() Task$log() Task$view() Task$set_solver() Task$set_scorer() Task$set_metrics() Task$get_cost() Task$clone()","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Creating and evaluating tasks — Task","text":"typical flow LLM evaluation vitals tends involve first calling method $eval() resulting object.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$new(   dataset,   solver,   scorer,   metrics = NULL,   epochs = NULL,   name = deparse(substitute(dataset)),   dir = vitals_log_dir() )"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"dataset tibble , minimally, columns input target. solver function takes vector inputs dataset's input column first argument determines values approximating dataset$target. return value must list following elements: result - character vector final responses, length dataset$input. solver_chat - list ellmer Chat objects used solve input, also length dataset$input. Additional output elements can included slot solver_metadata length dataset$input, logged solver_metadata. Additional arguments can passed solver via $solve(...) $eval(...). See definition generate() function outputs valid solver just passes inputs ellmer Chat objects' $chat() method parallel. scorer function evaluates well solver's return value approximates corresponding elements dataset$target. function take $get_samples() slot Task object return list following elements: score - vector scores length equal nrow(samples). Built-scorers return ordered factors levels < P (optionally) < C (standing \"Incorrect\", \"Partially Correct\", \"Correct\"). scorer returns output type, package automatically calculate metrics. Optionally: scorer_chat - scorer makes use ellmer, also include list ellmer Chat objects used score result, also length nrow(samples). scorer_metadata - intermediate results values like stored persistent log. also length equal nrow(samples). Scorers probably make use samples$input, samples$target, samples$result specifically. See model-based scoring examples. metrics named list functions take vector scores (task$get_samples()$score) output single numeric value. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new(). name name evaluation task. Defaults deparse(substitute(dataset)). dir Directory logs stored.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-eval-","dir":"Reference","previous_headings":"","what":"Method eval()","title":"Creating and evaluating tasks — Task","text":"Evaluates task running solver, scorer, logging results, viewing (interactive). method works calling $solve(), $score(), $log(), $view() sequence. typical flow LLM evaluation vitals tends involve first calling $new() method resulting object.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$eval(..., epochs = NULL, view = interactive())"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed solver scorer functions. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new(). view Automatically open viewer evaluation (defaults TRUE interactive, FALSE otherwise).","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-get-samples-","dir":"Reference","previous_headings":"","what":"Method get_samples()","title":"Creating and evaluating tasks — Task","text":"task's samples represent evaluation data frame format. vitals_bind() row-binds output function called across several tasks.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$get_samples()"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"tibble representing evaluation. Based dataset, epochs may duplicate rows, solver scorer append columns data.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-solve-","dir":"Reference","previous_headings":"","what":"Method solve()","title":"Creating and evaluating tasks — Task","text":"Solve task running solver","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$solve(..., epochs = NULL)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed solver function. epochs number times repeat sample. Evaluate sample multiple times better quantify variation. Optional, defaults 1L. value epochs supplied $eval() $score() take precedence value $new().","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-score-","dir":"Reference","previous_headings":"","what":"Method score()","title":"Creating and evaluating tasks — Task","text":"Score task running scorer applying metrics results.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$score(...)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"... Additional arguments passed scorer function.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-measure-","dir":"Reference","previous_headings":"","what":"Method measure()","title":"Creating and evaluating tasks — Task","text":"Applies metrics scored Task.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$measure()"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-log-","dir":"Reference","previous_headings":"","what":"Method log()","title":"Creating and evaluating tasks — Task","text":"Log task directory. Note , VITALS_LOG_DIR envvar set, happen automatically $eval().","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$log(dir = vitals_log_dir())"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"dir directory write log .","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"path logged file, invisibly.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-view-","dir":"Reference","previous_headings":"","what":"Method view()","title":"Creating and evaluating tasks — Task","text":"View task results Inspect log viewer","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$view()"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-set-solver-","dir":"Reference","previous_headings":"","what":"Method set_solver()","title":"Creating and evaluating tasks — Task","text":"Set solver function","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_solver(solver)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"solver function takes vector inputs dataset's input column first argument determines values approximating dataset$target. return value must list following elements: result - character vector final responses, length dataset$input. solver_chat - list ellmer Chat objects used solve input, also length dataset$input. Additional output elements can included slot solver_metadata length dataset$input, logged solver_metadata. Additional arguments can passed solver via $solve(...) $eval(...). See definition generate() function outputs valid solver just passes inputs ellmer Chat objects' $chat() method parallel.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-set-scorer-","dir":"Reference","previous_headings":"","what":"Method set_scorer()","title":"Creating and evaluating tasks — Task","text":"Set scorer function","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_scorer(scorer)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"scorer function evaluates well solver's return value approximates corresponding elements dataset$target. function take $get_samples() slot Task object return list following elements: score - vector scores length equal nrow(samples). Built-scorers return ordered factors levels < P (optionally) < C (standing \"Incorrect\", \"Partially Correct\", \"Correct\"). scorer returns output type, package automatically calculate metrics. Optionally: scorer_chat - scorer makes use ellmer, also include list ellmer Chat objects used score result, also length nrow(samples). scorer_metadata - intermediate results values like stored persistent log. also length equal nrow(samples). Scorers probably make use samples$input, samples$target, samples$result specifically. See model-based scoring examples.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task object (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-set-metrics-","dir":"Reference","previous_headings":"","what":"Method set_metrics()","title":"Creating and evaluating tasks — Task","text":"Set metrics applied $measure() (thus $eval()).","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$set_metrics(metrics)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"metrics named list functions take vector scores (task$get_samples()$score) output single numeric value.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"Task (invisibly)","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"Creating and evaluating tasks — Task","text":"cost eval wrapper around ellmer's $token_usage() function. function called beginning end call $solve() $score(); function returns cost inferred taking differences values $token_usage() time.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$get_cost()"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Creating and evaluating tasks — Task","text":"sum cost solving scoring evaluation. sum includes cost recent runs solver scorer.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Creating and evaluating tasks — Task","text":"objects class cloneable method.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating and evaluating tasks — Task","text":"","code":"Task$clone(deep = FALSE)"},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating and evaluating tasks — Task","text":"deep Whether make deep clone.","code":""},{"path":"https://vitals.tidyverse.org/reference/Task.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creating and evaluating tasks — Task","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    # evaluate the task (runs solver and scorer) and opens   # the results in the Inspect log viewer (if interactive)   tsk$eval() } #> ℹ Solving #> Error in ch$chat_parallel(as.list(inputs), ...): attempt to apply non-function #> ✖ Solving [199ms] #>"},{"path":"https://vitals.tidyverse.org/reference/are.html","id":null,"dir":"Reference","previous_headings":"","what":"An R Eval — are","title":"An R Eval — are","text":"R Eval dataset challenging R coding problems. input question R code solved first-read experts , chance read documentation run code, fluent data scientists. Solutions target() enable fluent data scientist evaluate whether solution deserves full, partial, credit. Pass dataset Task$new() situate inside evaluation task.","code":""},{"path":"https://vitals.tidyverse.org/reference/are.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An R Eval — are","text":"","code":"are"},{"path":"https://vitals.tidyverse.org/reference/are.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An R Eval — are","text":"tibble 26 rows 7 columns: id Character. Unique identifier/title code problem. input Character. question answered. target Character. solution, often description notable features correct solution. domain Character. technical domain (e.g., Data Analysis, Programming, Authoring). task Character. Type task (e.g., Debugging, New feature, Translation.) source Character. URL source problem. NAs indicate problem written originally eval. knowledge List. Required knowledge/concepts solving problem.","code":""},{"path":"https://vitals.tidyverse.org/reference/are.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"An R Eval — are","text":"Posit Community, GitHub issues, R4DS solutions, etc. row-level references, see source.","code":""},{"path":"https://vitals.tidyverse.org/reference/are.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"An R Eval — are","text":"","code":"are #> # A tibble: 26 × 7 #>    id                        input target domain task  source knowledge #>    <chr>                     <chr> <chr>  <chr>  <chr> <chr>  <list>    #>  1 after-stat-bar-heights    \"Thi… \"Pref… Data … New … https… <chr [1]> #>  2 conditional-grouped-summ… \"I h… \"One … Data … New … https… <chr [1]> #>  3 correlated-delays-reason… \"Her… \"Nota… Data … New … NA     <chr [1]> #>  4 curl-http-get             \"I h… \"Ther… Progr… Debu… https… <chr [1]> #>  5 dropped-level-legend      \"I'd… \"Also… Data … New … https… <chr [1]> #>  6 geocode-req-perform       \"I a… \"Form… Data … Debu… https… <chr [1]> #>  7 ggplot-breaks-feature     \"Her… \"```\\… Progr… New … https… <chr [2]> #>  8 grouped-filter-summarize  \"Her… \"Ther… Data … New … NA     <chr [1]> #>  9 grouped-mutate            \"I g… \"From… Data … Debu… https… <chr [1]> #> 10 implement-nse-arg         \"Add… \"```\\… Progr… New … https… <chr [1]> #> # ℹ 16 more rows  dplyr::glimpse(are) #> Rows: 26 #> Columns: 7 #> $ id        <chr> \"after-stat-bar-heights\", \"conditional-grouped-summ… #> $ input     <chr> \"This bar chart shows the count of different cuts o… #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n … #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", … #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"N… #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/d… #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"t…"},{"path":"https://vitals.tidyverse.org/reference/generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a chat to a solver function — generate","title":"Convert a chat to a solver function — generate","text":"generate() simplest possible solver one might use vitals; just passes inputs supplied model returns raw responses. inputs evaluated parallel, sense multiple R sessions, sense multiple, asynchronous HTTP requests using $chat_parallel(). generate()'s output can passed directory solver argument Task's $new() method.","code":""},{"path":"https://vitals.tidyverse.org/reference/generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a chat to a solver function — generate","text":"","code":"generate(solver_chat = NULL)"},{"path":"https://vitals.tidyverse.org/reference/generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a chat to a solver function — generate","text":"solver_chat ellmer chat object, ellmer::chat_anthropic().","code":""},{"path":"https://vitals.tidyverse.org/reference/generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a chat to a solver function — generate","text":"output generate() another function. function takes vector inputs, well solver chat name solver_chat default supplied generate() . See documentation solver argument Task information return type.","code":""},{"path":"https://vitals.tidyverse.org/reference/scorer_detect.html","id":null,"dir":"Reference","previous_headings":"","what":"Scoring with string detection — scorer_detect","title":"Scoring with string detection — scorer_detect","text":"following functions use string pattern detection score model outputs. detect_includes(): Determine whether target sample appears anywhere inside model output. Can case sensitive insensitive (defaults latter). detect_match(): Determine whether target sample appears beginning end model output (defaults looking end). options ignoring case, white-space, punctuation (ignored default). detect_pattern(): Extract matches pattern model response determine whether matches also appear target. detect_answer(): Scorer model output precedes answers \"ANSWER: \". Can extract letters, words, remainder line. detect_exact(): Scorer normalize text answer target(s) perform exact matching comparison text. scorer return CORRECT answer exact match one targets.","code":""},{"path":"https://vitals.tidyverse.org/reference/scorer_detect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scoring with string detection — scorer_detect","text":"","code":"detect_includes(case_sensitive = FALSE)  detect_match(   location = c(\"end\", \"begin\", \"end\", \"any\"),   case_sensitive = FALSE )  detect_pattern(pattern, case_sensitive = FALSE, all = FALSE)  detect_exact(case_sensitive = FALSE)  detect_answer(format = c(\"line\", \"word\", \"letter\"))"},{"path":"https://vitals.tidyverse.org/reference/scorer_detect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scoring with string detection — scorer_detect","text":"case_sensitive Logical, whether comparisons case sensitive. location look match: one \"begin\", \"end\", \"\", \"exact\". Defaults \"end\". pattern Regular expression pattern extract answer. Logical: multiple captures, whether must match. format extract \"ANSWER:\": \"letter\", \"word\", \"line\". Defaults \"line\".","code":""},{"path":"https://vitals.tidyverse.org/reference/scorer_detect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scoring with string detection — scorer_detect","text":"function scores model output based string matching. Pass returned value $eval(scorer). See documentation scorer argument Task information return type.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/reference/scorer_detect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scoring with string detection — scorer_detect","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # create a new Task   tsk <- Task$new(     dataset = simple_addition,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = detect_includes()   )    # evaluate the task (runs solver and scorer)   tsk$eval() } #> ℹ Solving #> Error in ch$chat_parallel(as.list(inputs), ...): attempt to apply non-function #> ✖ Solving [30ms] #>"},{"path":"https://vitals.tidyverse.org/reference/scorer_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Model-based scoring — scorer_model","title":"Model-based scoring — scorer_model","text":"Model-based scoring makes use model score output solver. model_graded_qa() scores well solver answers question/answer task. model_graded_fact() determines whether solver includes given fact response. two scorers quite similar implementation, use different default template evaluate correctness.","code":""},{"path":"https://vitals.tidyverse.org/reference/scorer_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model-based scoring — scorer_model","text":"","code":"model_graded_qa(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   scorer_chat = NULL )  model_graded_fact(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   scorer_chat = NULL )"},{"path":"https://vitals.tidyverse.org/reference/scorer_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model-based scoring — scorer_model","text":"template Grading template use–glue() string take substitutions input, answer, criterion, instructions. instructions Grading instructions. grade_pattern regex pattern extract final grade judge model's response. partial_credit Whether allow partial credit. scorer_chat ellmer chat used grade model output, e.g. ellmer::chat_anthropic().","code":""},{"path":"https://vitals.tidyverse.org/reference/scorer_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model-based scoring — scorer_model","text":"function grade model responses according given instructions. See Task's scorer argument description returned function. functions model_graded_qa() model_graded_fact() output can passed directly $eval(). See documentation scorer argument Task information return type.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/reference/scorer_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model-based scoring — scorer_model","text":"","code":"# Quality assurance ----------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- Task$new(     dataset = simple_addition,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_qa()   )    tsk$eval() } #> ℹ Solving #> Error in ch$chat_parallel(as.list(inputs), ...): attempt to apply non-function #> ✖ Solving [27ms] #>   # Factual response ------------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    r_history <- tibble(     input = c(       \"Who created the R programming language?\",       \"In what year was version 1.0 of R released?\"     ),     target = c(\"Ross Ihaka and Robert Gentleman.\", \"2000.\")   )    tsk <- Task$new(     dataset = r_history,     solver = generate(solver_chat = chat_anthropic(model = \"claude-3-7-sonnet-latest\")),     scorer = model_graded_fact()   )    tsk$eval() } #> ℹ Solving #> Error in ch$chat_parallel(as.list(inputs), ...): attempt to apply non-function #> ✖ Solving [27ms] #>"},{"path":"https://vitals.tidyverse.org/reference/vitals-package.html","id":null,"dir":"Reference","previous_headings":"","what":"vitals: Large Language Model Evaluation for R — vitals-package","title":"vitals: Large Language Model Evaluation for R — vitals-package","text":"port 'Inspect', widely adopted 'Python' framework large language model evaluation. Specifically aimed 'ellmer' users want measure effectiveness LLM-based apps, package includes facilities prompt engineering, tool usage, multi-turn dialog, model graded evaluations.","code":""},{"path":[]},{"path":"https://vitals.tidyverse.org/reference/vitals-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"vitals: Large Language Model Evaluation for R — vitals-package","text":"Maintainer: Simon Couch simon.couch@posit.co (ORCID) contributors: Posit Software, PBC (03wc8by49) [copyright holder, funder]","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bind.html","id":null,"dir":"Reference","previous_headings":"","what":"Concatenate task samples for analysis — vitals_bind","title":"Concatenate task samples for analysis — vitals_bind","text":"Combine multiple Task objects single tibble comparison. function takes multiple (optionally named) Task objects row-binds $get_samples() together, adding task column identify source row. resulting tibble nests additional columns metadata column ready analysis.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concatenate task samples for analysis — vitals_bind","text":"","code":"vitals_bind(...)"},{"path":"https://vitals.tidyverse.org/reference/vitals_bind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concatenate task samples for analysis — vitals_bind","text":"... Task objects combine, optionally named.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Concatenate task samples for analysis — vitals_bind","text":"tibble combined samples tasks, task column indicating source nested metadata column containing additional fields.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bundle.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare logs for deployment — vitals_bundle","title":"Prepare logs for deployment — vitals_bundle","text":"function creates standalone bundle Inspect viewer log files can deployed statically. copies UI viewer files, log files, generates necessary configuration files.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bundle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare logs for deployment — vitals_bundle","text":"","code":"vitals_bundle(log_dir = vitals_log_dir(), output_dir = NULL, overwrite = FALSE)"},{"path":"https://vitals.tidyverse.org/reference/vitals_bundle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare logs for deployment — vitals_bundle","text":"log_dir Path directory containing log files. Defaults vitals_log_dir(). output_dir Path directory bundled output placed. overwrite Whether overwrite existing output directory. Defaults FALSE.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_bundle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare logs for deployment — vitals_bundle","text":"Invisibly returns output directory path. directory contains:   robots.txt prevents crawlers indexing viewer. said, many crawlers read robots.txt root directory package, file likely ignored folder root directory deployed page. assets/ bundled source viewer. logs/ log_dir well logs.json, manifest file directory.","code":"output_dir |-- index.html |-- robots.txt |-- assets     |--  .. |-- logs     |--  .."},{"path":"https://vitals.tidyverse.org/reference/vitals_bundle.html","id":"deployment","dir":"Reference","previous_headings":"","what":"Deployment","title":"Prepare logs for deployment — vitals_bundle","text":"function generates directory ready deployment static web server GitHub Pages, S3 buckets, Netlify. connection Posit Connect configured, can deploy directory log files following:","code":"tmp_dir <- withr::local_tempdir() vitals_bundle(output_dir = tmp_dir, overwrite = TRUE) rsconnect::deployApp(tmp_dir)"},{"path":"https://vitals.tidyverse.org/reference/vitals_log_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"The log directory — vitals_log_dir","title":"The log directory — vitals_log_dir","text":"vitals supports VITALS_LOG_DIR environment variable, sets default directory write logs Task's $eval() $log() methods.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_log_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The log directory — vitals_log_dir","text":"","code":"vitals_log_dir()  vitals_log_dir_set(dir)"},{"path":"https://vitals.tidyverse.org/reference/vitals_log_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The log directory — vitals_log_dir","text":"dir directory configure environment variable VITALS_LOG_DIR .","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_log_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The log directory — vitals_log_dir","text":"vitals_log_dir() vitals_log_dir_set() return current value environment variable VITALS_LOG_DIR. vitals_log_dir_set() additionally sets new value. set variable every new R session, might consider adding .Rprofile, perhaps usethis::edit_r_profile().","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_log_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The log directory — vitals_log_dir","text":"","code":"vitals_log_dir() #> [1] \"$RUNNER_TEMP\"  dir <- tempdir()  vitals_log_dir_set(dir)  vitals_log_dir() #> [1] \"/tmp/RtmpmDLaur\""},{"path":"https://vitals.tidyverse.org/reference/vitals_view.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactively view local evaluation logs — vitals_view","title":"Interactively view local evaluation logs — vitals_view","text":"vitals bundles Inspect log viewer, interactive app exploring evaluation logs. Supply path directory tasks written json. individual Task objects, use $view() method instead.","code":""},{"path":"https://vitals.tidyverse.org/reference/vitals_view.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactively view local evaluation logs — vitals_view","text":"","code":"vitals_view(dir = vitals_log_dir(), host = \"127.0.0.1\", port = 7576)"},{"path":"https://vitals.tidyverse.org/reference/vitals_view.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactively view local evaluation logs — vitals_view","text":"dir Path directory containing task eval logs. host Host serve . Defaults \"127.0.0.1\". port Port serve . Defaults 7576, one greater Python implementation.","code":""}]
