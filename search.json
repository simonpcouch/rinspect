[{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others‚Äô private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://simonpcouch.github.io/rinspect/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla‚Äôs code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://simonpcouch.github.io/rinspect/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 rinspect authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"an-r-eval-dataset","dir":"Articles","previous_headings":"","what":"An R eval dataset","title":"Getting started with rinspect","text":"docs: R Eval dataset challenging R coding problems. input question R code solved first-read human experts , chance read documentation run code, fluent data scientists. Solutions target enable fluent data scientist evaluate whether solution deserves full, partial, credit. high level: title: unique identifier problem. input: question answered. target: solution, often description notable features correct solution. domain, task, knowledge pieces metadata describing kind R coding challenge. source: problem came , URL. Many coding problems adapted ‚Äúwild‚Äù include kinds context usually available answering questions. purposes actually carrying initial evaluation, ‚Äôre specifically interested input target columns. Let‚Äôs print first entry full can get taste typical problem dataset: ‚Äôs suggested solution:","code":"glimpse(are) #> Rows: 29 #> Columns: 7 #> $ title     <chr> \"after-stat-bar-heights\", \"conditional-grouped-summary\", \"co‚Ä¶ #> $ input     <chr> \"This bar chart shows the count of different cuts of diamond‚Ä¶ #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n  geom_bar‚Ä¶ #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", \"Programm‚Ä¶ #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"New code\",‚Ä¶ #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/data-visua‚Ä¶ #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"tidyverse\"‚Ä¶ cat(are$input[1]) #> This bar chart shows the count of different cuts of diamonds, and each #> bar is stacked and filled according to clarity: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(mapping = aes(x = cut, fill = clarity)) #> ``` #>  #> Could you change this code so that the proportion of diamonds with a #> given cut corresponds to the bar height and not the count? Each bar #> should still be filled according to clarity. cat(are$target[1]) #> Preferably: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(aes(x = cut, y = after_stat(count) / sum(after_stat(count)), #> fill = clarity)) #> ``` #>  #> The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0, but #> it still works: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = clarity)) #> ``` #>  #> Simply setting `position = \"fill\" will result in each bar having a #> height of 1 and is not correct."},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"creating-an-evaluation-task","dir":"Articles","previous_headings":"","what":"Creating an evaluation task","title":"Getting started with rinspect","text":"task_create() situates dataset inside evaluation task, subclass tibble additional attributes track experiment. dataset fair game long columns input target. name argument optional, ‚Äôll specify : notable change user new id column, just row number.","code":"are_task <- task_create(dataset = are, name = \"An R Eval\") are_task #> # Evaluation task An R Eval. #> # A tibble: 29 √ó 8 #>    title                       input  target domain task  source knowledge    id #>  * <chr>                       <chr>  <chr>  <chr>  <chr> <chr>  <list>    <int> #>  1 after-stat-bar-heights      \"This‚Ä¶ \"Pref‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]>     1 #>  2 conditional-grouped-summary \"I ha‚Ä¶ \"One ‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]>     2 #>  3 correlated-delays-reasoning \"Here‚Ä¶ \"Nota‚Ä¶ Data ‚Ä¶ New ‚Ä¶ NA     <chr [1]>     3 #>  4 curl-http-get               \"I ha‚Ä¶ \"Ther‚Ä¶ Progr‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]>     4 #>  5 dropped-level-legend        \"I'd ‚Ä¶ \"Also‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]>     5 #>  6 geocode-req-perform         \"I am‚Ä¶ \"Form‚Ä¶ Data ‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]>     6 #>  7 ggplot-breaks-feature       \"Here‚Ä¶ \"```\\‚Ä¶ Progr‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]>     7 #>  8 ggplot-breaks-feature       \"Here‚Ä¶ \"```\\‚Ä¶ Progr‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]>     8 #>  9 grouped-filter-summarize    \"Here‚Ä¶ \"Ther‚Ä¶ Data ‚Ä¶ New ‚Ä¶ NA     <chr [1]>     9 #> 10 grouped-mutate              \"I ge‚Ä¶ \"From‚Ä¶ Data ‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]>    10 #> # ‚Ñπ 19 more rows"},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"solving-the-task-with-claude","dir":"Articles","previous_headings":"","what":"Solving the task with Claude","title":"Getting started with rinspect","text":"Next, ‚Äôll use task_solve() apply solver task. solver function takes input question produces result. basic solver rinspect plain ellmer chat, like chat_claude(). (ellmer chats indeed functions; rinspect special-cases .) Users can supply function please, though, likely wraps calls ellmer chats way. example, though, ‚Äôll just use chat_claude(). time writing, current default model used chat_claude() Claude Sonnet 3.5. input dataset sent Claude. input prompt, Claude generates response attempts solve R coding problem. task_solve() append two columns, output solver. ‚Äôs model responded first question : output final response solver, solver ellmer chat object led output. ‚Äôs technically required solvers provide chats output, required ‚Äôd like explore evaluation results inspect_view(), highly recommended.","code":"are_solved <- task_solve(are_task, solver = chat_claude()) cat(are_solved$output[1]) #> Yes! To show proportions instead of counts, you can add `position = #> \"fill\"` to the `geom_bar()` function. This will normalize each bar to #> show the relative proportions of clarity within each cut category. #> Here's the modified code: #>  #> ``` #> ggplot(data = diamonds) + #> geom_bar(mapping = aes(x = cut, fill = clarity), position = \"fill\") #> ``` #>  #> This will create a stacked bar chart where each bar has the same height #> (1.0 or 100%), and the segments within each bar show the proportion of #> diamonds with each clarity grade for that particular cut."},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"scoring-the-solutions","dir":"Articles","previous_headings":"","what":"Scoring the solutions","title":"Getting started with rinspect","text":"Now ‚Äôll evaluate quality Claude‚Äôs solutions using task_score() model_graded_qa() scorer: model_graded_qa() model-graded scorer provides package. step compare Claude‚Äôs solutions reference solutions provided target column, assigning score solution using another model. score either 1 0, though since ‚Äôve set partial_credit = TRUE, model can also choose allot response .5. rinspect use model generate final response task_solve() model score solutions. Hold , though‚Äî‚Äôre using LLM generate responses questions, using LLM grade responses?  technique called ‚Äúmodel grading‚Äù ‚ÄúLLM---judge.‚Äù Done correctly, model grading effective scalable solution scoring. said, ‚Äôs without faults. ‚Äôs grading model thought response:","code":"are_scored <- task_score(   are_solved,    scorer = model_graded_qa(partial_credit = TRUE) ) cat(are_scored$scorer[[1]]$last_turn()@text) #> Let me analyze this: #>  #> The criterion specifies that we want to show the overall proportion of #> diamonds for each cut (i.e., how many diamonds of each cut there are #> relative to the total number of diamonds), while still maintaining the #> clarity breakdown within each cut. #>  #> The submitted answer suggests using `position = \"fill\"`, which actually #> does something different - it normalizes each bar to a height of 1 (or #> 100%) and shows the relative proportions of clarity within each cut. #> This means each bar will have the same height regardless of how common #> that cut is in the dataset. #>  #> This is incorrect because: #> 1. It doesn't show the true proportion of each cut in the dataset #> 2. Using `position = \"fill\"` standardizes each bar to height 1, which #> masks the actual proportional differences between cuts #>  #> The correct solution (as shown in the criterion) should use #> `after_stat(count) / sum(after_stat(count))` to calculate the true #> proportion of each cut relative to the total number of diamonds. #>  #> The submission provides a solution that creates a different #> visualization than what was asked for, showing within-cut proportions #> rather than overall proportions. #>  #> GRADE: I"},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"analyzing-the-results","dir":"Articles","previous_headings":"","what":"Analyzing the results","title":"Getting started with rinspect","text":"Especially first times run eval, ‚Äôll want inspect (ha!) outputs closely. rinspect package ships app, Inspect log viewer, allows drill solutions grading decisions model sample. first couple runs, ‚Äôll likely find revisions can make grading guidance target align model responses intent. Interactively, ‚Äôd launch app inspect_view(are_scored). can (using eval specifically!) following code (TODO: code grab github view). cursory analysis, can visualize correct vs.¬†partially correct vs.¬†incorrect answers:  Claude answered fully correctly 14 29 samples, partially correctly 8 times. , leads sorts questions: models cheaper Claude just well? even local model? models available better box? Claude better allow ‚Äúreason‚Äù briefly answering? Claude better gave tools ‚Äôd allow peruse documentation /run R code answering? (See btw::register_btw_tools() ‚Äôre interested .) ‚Äôre interested questions well, check vignettes package!","code":"are_scored <-    are_scored %>%    mutate(grade = case_when(     score == 1 ~ \"Correct\",     score == .5 ~ \"Partially Correct\",     score == 0 ~ \"Incorrect\"     ),     grade = factor(grade, levels = c(\"Correct\", \"Partially Correct\", \"Incorrect\"))   )  are_scored %>%   ggplot() +   aes(x = grade) +   geom_bar()"},{"path":"https://simonpcouch.github.io/rinspect/articles/rinspect.html","id":"saving-and-loading-evaluation-results","dir":"Articles","previous_headings":"","what":"Saving and loading evaluation results","title":"Getting started with rinspect","text":"storing evaluation results later use, couple options: Complete: tasks just tibbles, can saved .rda .rds files using usual save() saveRDS(). Tasks saved way can reloaded back R passed inspect_view(). Portable: hood, inspect_view() task, ‚Äôs written temporary .json file Inspect log viewer can read. can write permanent (portable .rda) viewer-compatible .json file inspect_log(). Tasks can‚Äôt read back R .json directly, can still view log viewer inspect_view().","code":""},{"path":"https://simonpcouch.github.io/rinspect/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://simonpcouch.github.io/rinspect/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). rinspect: Large Language Model Evaluation R. R package version 0.0.0.9000, https://simonpcouch.github.io/rinspect/, https://github.com/simonpcouch/rinspect.","code":"@Manual{,   title = {rinspect: Large Language Model Evaluation for R},   author = {Simon Couch},   year = {2025},   note = {R package version 0.0.0.9000,     https://simonpcouch.github.io/rinspect/},   url = {https://github.com/simonpcouch/rinspect}, }"},{"path":"https://simonpcouch.github.io/rinspect/index.html","id":"rinspect","dir":"","previous_headings":"","what":"Large Language Model Evaluation for R","title":"Large Language Model Evaluation for R","text":"rinspect framework large language model evaluation R. ‚Äôs specifically aimed ellmer users want measure effectiveness LLM-based apps. package R port widely adopted Python framework Inspect. package doesn‚Äôt integrate Inspect directly, allows users interface Inspect Log Viewer provides -ramp transition Inspect need writing evaluation logs file format. Important üöß construction! üöß rinspect highly experimental much documentation aspirational.","code":""},{"path":"https://simonpcouch.github.io/rinspect/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Large Language Model Evaluation for R","text":"can install developmental version rinspect using:","code":"pak::pak(\"simonpcouch/rinspect\")"},{"path":"https://simonpcouch.github.io/rinspect/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Large Language Model Evaluation for R","text":"Inspect based three core concepts: datasets, solvers, scorers. rinspect, objects act tasks. Datasets set labelled samples input prompt target either literal value(s) grading guidance. Situate dataset inside task task_create(): Solvers evaluate input dataset produce final result (hopefully) resembling target. simplest example solver plain ellmer chat‚Äîevaluate solver task task_solve(): Scorers evaluate final output solvers. may use text comparisons, model grading, custom schemes. Score solver output using task_score(): task scored, ‚Äôs ready explore interactively Inspect log viewer:","code":"library(rinspect) library(ellmer) library(tibble) simple_addition <- tibble(   input = c(\"What's 2+2?\", \"What's 2+3?\", \"What's 2+4?\"),   target = c(\"4\", \"5\", \"6\") )  tsk <- task_create(dataset = simple_addition) tsk #> # Evaluation task simple_addition. #> # A tibble: 3 √ó 3 #>   input       target    id #> * <chr>       <chr>  <int> #> 1 What's 2+2? 4          1 #> 2 What's 2+3? 5          2 #> 3 What's 2+4? 6          3 tsk <- task_solve(tsk, solver = chat_claude()) #> Solving ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                       33% | ETA:  2s #> Solving ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             67% | ETA:  1s #> Solving ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100% | ETA:  0s tsk #> # Evaluation task simple_addition. #> # A tibble: 3 √ó 5 #>   input       target    id output  solver #> * <chr>       <chr>  <int> <chr>   <list> #> 1 What's 2+2? 4          1 2+2=4   <Chat> #> 2 What's 2+3? 5          2 2+3 = 5 <Chat> #> 3 What's 2+4? 6          3 2+4 = 6 <Chat> tsk <- task_score(tsk, scorer = model_graded_qa()) #> Scoring ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                       33% | ETA:  5s #> Scoring ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†             67% | ETA:  3s #> Scoring ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100% | ETA:  0s tsk #> # Evaluation task simple_addition. #> # A tibble: 3 √ó 7 #>   input       target    id output  solver score scorer #> * <chr>       <chr>  <int> <chr>   <list> <dbl> <list> #> 1 What's 2+2? 4          1 2+2=4   <Chat>     1 <Chat> #> 2 What's 2+3? 5          2 2+3 = 5 <Chat>     1 <Chat> #> 3 What's 2+4? 6          3 2+4 = 6 <Chat>     1 <Chat> inspect_view(tsk)"},{"path":"https://simonpcouch.github.io/rinspect/reference/are.html","id":null,"dir":"Reference","previous_headings":"","what":"An R Eval ‚Äî are","title":"An R Eval ‚Äî are","text":"R Eval dataset challenging R coding problems. input question R code solved first-read experts , chance read documentation run code, fluent data scientists. Solutions target() enable fluent data scientist evaluate whether solution deserves full, partial, credit. Pass dataset task_create() situate inside evaluation task.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/are.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An R Eval ‚Äî are","text":"","code":"are"},{"path":"https://simonpcouch.github.io/rinspect/reference/are.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An R Eval ‚Äî are","text":"tibble 29 rows 7 columns: title Character. Unique identifier/title code problem. input Character. question answered. target Character. solution, often description notable features correct solution. domain Character. technical domain (e.g., Data Analysis, Programming, Authoring). task Character. Type task (e.g., Debugging, New feature, Translation.) source Character. URL source problem. NAs indicate problem written originally eval. knowledge List. Required knowledge/concepts solving problem.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/are.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"An R Eval ‚Äî are","text":"Posit Community, GitHub issues, R4DS solutions, etc. row-level references, see source.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/are.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"An R Eval ‚Äî are","text":"","code":"are #> # A tibble: 29 √ó 7 #>    title                       input        target domain task  source knowledge #>    <chr>                       <chr>        <chr>  <chr>  <chr> <chr>  <list>    #>  1 after-stat-bar-heights      \"This bar c‚Ä¶ \"Pref‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]> #>  2 conditional-grouped-summary \"I have a s‚Ä¶ \"One ‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]> #>  3 correlated-delays-reasoning \"Here's som‚Ä¶ \"Nota‚Ä¶ Data ‚Ä¶ New ‚Ä¶ NA     <chr [1]> #>  4 curl-http-get               \"I have the‚Ä¶ \"Ther‚Ä¶ Progr‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]> #>  5 dropped-level-legend        \"I'd like t‚Ä¶ \"Also‚Ä¶ Data ‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]> #>  6 geocode-req-perform         \"I am tryin‚Ä¶ \"Form‚Ä¶ Data ‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]> #>  7 ggplot-breaks-feature       \"Here's a f‚Ä¶ \"```\\‚Ä¶ Progr‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]> #>  8 ggplot-breaks-feature       \"Here's a f‚Ä¶ \"```\\‚Ä¶ Progr‚Ä¶ New ‚Ä¶ https‚Ä¶ <chr [1]> #>  9 grouped-filter-summarize    \"Here's som‚Ä¶ \"Ther‚Ä¶ Data ‚Ä¶ New ‚Ä¶ NA     <chr [1]> #> 10 grouped-mutate              \"I get an e‚Ä¶ \"From‚Ä¶ Data ‚Ä¶ Debu‚Ä¶ https‚Ä¶ <chr [1]> #> # ‚Ñπ 19 more rows  dplyr::glimpse(are) #> Rows: 29 #> Columns: 7 #> $ title     <chr> \"after-stat-bar-heights\", \"conditional-grouped-summary\", \"co‚Ä¶ #> $ input     <chr> \"This bar chart shows the count of different cuts of diamond‚Ä¶ #> $ target    <chr> \"Preferably: \\n\\n```\\nggplot(data = diamonds) + \\n  geom_bar‚Ä¶ #> $ domain    <chr> \"Data analysis\", \"Data analysis\", \"Data analysis\", \"Programm‚Ä¶ #> $ task      <chr> \"New code\", \"New code\", \"New code\", \"Debugging\", \"New code\",‚Ä¶ #> $ source    <chr> \"https://jrnold.github.io/r4ds-exercise-solutions/data-visua‚Ä¶ #> $ knowledge <list> \"tidyverse\", \"tidyverse\", \"tidyverse\", \"r-lib\", \"tidyverse\"‚Ä¶"},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a task to an eval log ‚Äî inspect_log","title":"Write a task to an eval log ‚Äî inspect_log","text":"function translates task evaluation log file readable Inspect log viewer. usual entry point function inspect_view(); use inspect_log() write persistent log task eval. said, evaluation logs read back R tasks later analyzed (viewed Inspect log viewer, matter); use, likely want save task using save() saveRDS().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a task to an eval log ‚Äî inspect_log","text":"","code":"inspect_log(task, dir = attr(res, \"dir\"))"},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a task to an eval log ‚Äî inspect_log","text":"task evaluation task created task_create(). dir Directory logs stored.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a task to an eval log ‚Äî inspect_log","text":"path directory log written . Pass value inspect_view() view task logs.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"The log directory ‚Äî inspect_log_dir","title":"The log directory ‚Äî inspect_log_dir","text":"rinspect supports INSPECT_LOG_DIR environment variable, sets default directory write logs task_create() inspect_log() view inspect_view().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The log directory ‚Äî inspect_log_dir","text":"","code":"inspect_log_dir()  inspect_log_dir_set(dir)"},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The log directory ‚Äî inspect_log_dir","text":"dir Directory logs stored.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The log directory ‚Äî inspect_log_dir","text":"inspect_log_dir() inspect_log_dir_set() return current value environment variable INSPECT_LOG_DIR. inspect_log_dir_set() additionally sets new value.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_log_dir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The log directory ‚Äî inspect_log_dir","text":"","code":"inspect_log_dir() #> [1] NA  dir <- tempdir()  inspect_log_dir_set(dir)  inspect_log_dir() #> [1] \"/tmp/RtmpGLUEHR\""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_view.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive evaluation log viewer ‚Äî inspect_view","title":"Interactive evaluation log viewer ‚Äî inspect_view","text":"rinspect bundles Inspect log viewer, interactive app exploring evaluation logs. can view tasks directly, write task temporary json file, can supply path directory tasks written json using inspect_log().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_view.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive evaluation log viewer ‚Äî inspect_view","text":"","code":"inspect_view(x, host = \"127.0.0.1\", port = 7576)  # S3 method for class 'character' inspect_view(x, host = \"127.0.0.1\", port = 7576)  # S3 method for class 'task' inspect_view(x, host = \"127.0.0.1\", port = 7576)"},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_view.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive evaluation log viewer ‚Äî inspect_view","text":"x Either path directory containing task eval log task . task, function log task temporary directory open directory viewer. host Host serve . Defaults \"127.0.0.1\", port Port serve . Defaults 7576, one greater Python implementation.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/inspect_view.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive evaluation log viewer ‚Äî inspect_view","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- task_create(dataset = simple_addition)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_qa())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%"},{"path":"https://simonpcouch.github.io/rinspect/reference/rinspect-package.html","id":null,"dir":"Reference","previous_headings":"","what":"rinspect: Large Language Model Evaluation for R ‚Äî rinspect-package","title":"rinspect: Large Language Model Evaluation for R ‚Äî rinspect-package","text":"port 'Inspect', widely adopted 'Python' framework large language model evaluation. Specifically aimed 'ellmer' users want measure effectiveness LLM-based apps, package includes facilities prompt engineering, tool usage, multi-turn dialog, model graded evaluations.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/rinspect-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"rinspect: Large Language Model Evaluation for R ‚Äî rinspect-package","text":"Maintainer: Simon Couch simon.couch@posit.co (ORCID) contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_detect.html","id":null,"dir":"Reference","previous_headings":"","what":"Scoring with string detection ‚Äî scorer_detect","title":"Scoring with string detection ‚Äî scorer_detect","text":"Functions string pattern detection model outputs: detect_includes(): Determine whether target sample appears anywhere inside model output. Can case sensitive insensitive (defaults latter). detect_match(): Determine whether target sample appears beginning end model output (defaults looking end). options ignoring case, white-space, punctuation (ignored default). detect_pattern(): Extract matches pattern model response determine whether matches also appear target. detect_answer(): Scorer model output precedes answers \"ANSWER: \". Can extract letters, words, remainder line. detect_exact(): Scorer normalize text answer target(s) perform exact matching comparison text. scorer return CORRECT answer exact match one targets.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_detect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scoring with string detection ‚Äî scorer_detect","text":"","code":"detect_includes(case_sensitive = FALSE)  detect_match(   location = c(\"end\", \"begin\", \"end\", \"any\"),   case_sensitive = FALSE )  detect_pattern(pattern, case_sensitive = FALSE, all = FALSE)  detect_exact(case_sensitive = FALSE)  detect_answer(format = c(\"line\", \"word\", \"letter\"))"},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_detect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scoring with string detection ‚Äî scorer_detect","text":"case_sensitive Logical, whether comparisons case sensitive. location look match: one \"begin\", \"end\", \"\", \"exact\". Defaults \"end\". pattern Regular expression pattern extract answer. Logical: multiple captures, whether must match. format extract \"ANSWER:\": \"letter\", \"word\", \"line\". Defaults \"line\".","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_detect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scoring with string detection ‚Äî scorer_detect","text":"function scores model output based string matching. Pass returned value task_score().","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_detect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scoring with string detection ‚Äî scorer_detect","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    # Requires an API key:   tsk <- task_create(dataset = simple_addition)   tsk <- task_solve(tsk, solver = chat_claude())    # Does not require an API key:   tsk <- task_score(tsk, scorer = detect_includes())   tsk    if (interactive()) {     inspect_view(tsk)   } }"},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Model-based scoring ‚Äî scorer_model","title":"Model-based scoring ‚Äî scorer_model","text":"Model-based scoring makes use model score output solver. model_graded_qa() scores well solver answers question/answer task. model_graded_fact() determines whether solver includes given fact response. two scorers quite similar implementation, use different default template evaluate correctness.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model-based scoring ‚Äî scorer_model","text":"","code":"model_graded_qa(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   chat = NULL )  model_graded_fact(   template = NULL,   instructions = NULL,   grade_pattern = \"(?i)GRADE\\\\s*:\\\\s*([CPI])(.*)$\",   partial_credit = FALSE,   chat = NULL )"},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model-based scoring ‚Äî scorer_model","text":"template Grading template use‚Äìglue() string take substitutions input, answer, criterion, instructions. instructions Grading instructions. grade_pattern regex pattern extract final grade judge model's response. partial_credit Whether allow partial credit. chat ellmer chat used grade model output, e.g. ellmer::chat_claude().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model-based scoring ‚Äî scorer_model","text":"function grade model responses according given instructions. returned function takes solved task outputs 2-element list, first element scores vector scores length nrow(task) second list ellmer chats led scores, also length nrow(task). function model_graded_qa()'s outputs can passed directly task_score().","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/scorer_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model-based scoring ‚Äî scorer_model","text":"","code":"# Quality assurance ----------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- task_create(dataset = simple_addition)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_qa())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%  # Factual response ------------------------------- if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    r_history <- tibble(     input = c(       \"Who created the R programming language?\",       \"In what year was version 1.0 of R released?\"     ),     target = c(\"Ross Ihaka and Robert Gentleman.\", \"2000.\")   )    tsk <- task_create(dataset = r_history)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_fact())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100% #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Creating tasks ‚Äî task_create","title":"Creating tasks ‚Äî task_create","text":"Evaluation tasks provide flexible data structure evaluating LLM-based tools. Datasets contain set labelled samples. Datasets just tibble columns input target, input prompt target either literal value(s) grading guidance. Situate datasets inside task task_create(). Solvers evaluate input dataset produce final result. simplest solver just ellmer chat (e.g. ellmer::chat_claude()). Evaluate task solver using task_solve(). Scorers evaluate final output solvers. may use text comparisons (like detect_match()), model grading (like model_graded_qa()), custom schemes. Score solver results using task_score().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creating tasks ‚Äî task_create","text":"","code":"task_create(   dataset,   name = deparse(substitute(dataset)),   dir = inspect_log_dir() )"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creating tasks ‚Äî task_create","text":"dataset tibble , minimally, columns input target. name name evaluation task. Defaults deparse(substitute(dataset)). dir Directory logs stored.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_create.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creating tasks ‚Äî task_create","text":"task object, subclass tibble. task_create() appends column id input.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/task_create.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creating tasks ‚Äî task_create","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- task_create(dataset = simple_addition)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_qa())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100% #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Scoring tasks ‚Äî task_score","title":"Scoring tasks ‚Äî task_score","text":"Evaluation tasks provide flexible data structure evaluating LLM-based tools. Datasets contain set labelled samples. Datasets just tibble columns input target, input prompt target either literal value(s) grading guidance. Situate datasets inside task task_create(). Solvers evaluate input dataset produce final result. simplest solver just ellmer chat (e.g. ellmer::chat_claude()). Evaluate task solver using task_solve(). Scorers evaluate final output solvers. may use text comparisons (like detect_match()), model grading (like model_graded_qa()), custom schemes. Score solver results using task_score().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scoring tasks ‚Äî task_score","text":"","code":"task_score(task, scorer)"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scoring tasks ‚Äî task_score","text":"task evaluation task created task_create(). scorer function evaluates well solver's return value approximates corresponding elements dataset$target. See model-based scoring examples.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scoring tasks ‚Äî task_score","text":"task object, subclass tibble. task_score() appends columns score scorer input.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/task_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scoring tasks ‚Äî task_score","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- task_create(dataset = simple_addition)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_qa())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_solve.html","id":null,"dir":"Reference","previous_headings":"","what":"Solving tasks ‚Äî task_solve","title":"Solving tasks ‚Äî task_solve","text":"Evaluation tasks provide flexible data structure evaluating LLM-based tools. Datasets contain set labelled samples. Datasets just tibble columns input target, input prompt target either literal value(s) grading guidance. Situate datasets inside task task_create(). Solvers evaluate input dataset produce final result. simplest solver just ellmer chat (e.g. ellmer::chat_claude()). Evaluate task solver using task_solve(). Scorers evaluate final output solvers. may use text comparisons (like detect_match()), model grading (like model_graded_qa()), custom schemes. Score solver results using task_score().","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_solve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solving tasks ‚Äî task_solve","text":"","code":"task_solve(task, solver, epochs = 1L)"},{"path":"https://simonpcouch.github.io/rinspect/reference/task_solve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solving tasks ‚Äî task_solve","text":"task evaluation task created task_create(). solver function takes vector dataset$input first argument determines value approximating dataset$target. return value list elements outputs (vector final responses, length dataset$input) solvers (list ellmer chats used solve inputs, also length dataset$input). , just supply ellmer chat (e.g. ellmer::chat_claude()) rinspect take care details. epochs number times repeat sample. Evaluate sample multiple times measure variation. Optional, defaults 1L.","code":""},{"path":"https://simonpcouch.github.io/rinspect/reference/task_solve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solving tasks ‚Äî task_solve","text":"task object, subclass tibble. task_solve() appends columns output, solver, (equal 1L) epoch input.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/rinspect/reference/task_solve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Solving tasks ‚Äî task_solve","text":"","code":"if (!identical(Sys.getenv(\"ANTHROPIC_API_KEY\"), \"\")) {   library(ellmer)   library(tibble)    simple_addition <- tibble(     input = c(\"What's 2+2?\", \"What's 2+3?\"),     target = c(\"4\", \"5\")   )    tsk <- task_create(dataset = simple_addition)   tsk    tsk <- task_solve(tsk, solver = chat_claude())   tsk    tsk <- task_score(tsk, scorer = model_graded_qa())   tsk    if (interactive()) {     inspect_view(tsk)   } } #> [working] (0 + 0) -> 1 -> 1 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†                  50% #> [working] (0 + 0) -> 0 -> 2 | ‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†  100%"}]
