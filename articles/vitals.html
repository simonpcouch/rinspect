<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Getting started with vitals • vitals</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Getting started with vitals">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">vitals</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9002</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/vitals.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/solvers.html">Custom solvers and tool calling</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidyverse/vitals/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Getting started with vitals</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidyverse/vitals/blob/main/vignettes/vitals.Rmd" class="external-link"><code>vignettes/vitals.Rmd</code></a></small>
      <div class="d-none name"><code>vitals.Rmd</code></div>
    </div>

    
    
<p>At their core, LLM evals are composed of three pieces:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Datasets</strong> contain a set of labelled samples.
Datasets are just a tibble with columns <code>input</code> and
<code>target</code>, where <code>input</code> is a prompt and
<code>target</code> is either literal value(s) or grading guidance.</li>
<li>
<strong>Solvers</strong> evaluate the <code>input</code> in the
dataset and produce a final result (hopefully) approximating
<code>target</code>. In vitals, the simplest solver is just an ellmer
chat (e.g. <code><a href="https://ellmer.tidyverse.org/reference/chat_anthropic.html" class="external-link">ellmer::chat_anthropic()</a></code>) wrapped in
<code><a href="../reference/generate.html">generate()</a></code>,
i.e. <code>generate(ellmer::chat_anthropic()</code>), which will call
the Chat object’s <code>$chat()</code> method and return whatever it
returns.</li>
<li>
<strong>Scorers</strong> evaluate the final output of solvers. They
may use text comparisons, model grading, or other custom schemes to
determine how well the solver approximated the <code>target</code> based
on the <code>input</code>.</li>
</ol>
<p>This vignette will explore these three components using
<code>are</code>, an example dataset that ships with the package.</p>
<p>First, load the required packages:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/vitals" class="external-link">vitals</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ellmer.tidyverse.org" class="external-link">ellmer</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="an-r-eval-dataset">An R eval dataset<a class="anchor" aria-label="anchor" href="#an-r-eval-dataset"></a>
</h2>
<p>From the <code>are</code> docs:</p>
<blockquote>
<p>An R Eval is a dataset of challenging R coding problems. Each
<code>input</code> is a question about R code which could be solved on
first-read only by human experts and, with a chance to read
documentation and run some code, by fluent data scientists. Solutions
are in <code>target</code> and enable a fluent data scientist to
evaluate whether the solution deserves full, partial, or no credit.</p>
</blockquote>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html" class="external-link">glimpse</a></span><span class="op">(</span><span class="va">are</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; Rows: 26</span></span>
<span><span class="co">#&gt; Columns: 7</span></span>
<span><span class="co">#&gt; $ id        <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "after-stat-bar-heights"<span style="color: #949494;">, </span>"conditional-grouped-summary"<span style="color: #949494;">, </span>"co…</span></span>
<span><span class="co">#&gt; $ input     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "This bar chart shows the count of different cuts of diamond…</span></span>
<span><span class="co">#&gt; $ target    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "Preferably: \n\n```\nggplot(data = diamonds) + \n  geom_bar…</span></span>
<span><span class="co">#&gt; $ domain    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "Data analysis"<span style="color: #949494;">, </span>"Data analysis"<span style="color: #949494;">, </span>"Data analysis"<span style="color: #949494;">, </span>"Programm…</span></span>
<span><span class="co">#&gt; $ task      <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "New code"<span style="color: #949494;">, </span>"New code"<span style="color: #949494;">, </span>"New code"<span style="color: #949494;">, </span>"Debugging"<span style="color: #949494;">, </span>"New code"<span style="color: #949494;">,</span>…</span></span>
<span><span class="co">#&gt; $ source    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "https://jrnold.github.io/r4ds-exercise-solutions/data-visua…</span></span>
<span><span class="co">#&gt; $ knowledge <span style="color: #949494; font-style: italic;">&lt;list&gt;</span> "tidyverse"<span style="color: #949494;">, </span>"tidyverse"<span style="color: #949494;">, </span>"tidyverse"<span style="color: #949494;">, </span>"r-lib"<span style="color: #949494;">, </span>"tidyverse"…</span></span></code></pre>
<p>At a high level:</p>
<ul>
<li>
<code>id</code>: A unique identifier for the problem.</li>
<li>
<code>input</code>: The question to be answered.</li>
<li>
<code>target</code>: The solution, often with a description of
notable features of a correct solution.</li>
<li>
<code>domain</code>, <code>task</code>, and <code>knowledge</code>
are pieces of metadata describing the kind of R coding challenge.</li>
<li>
<code>source</code>: Where the problem came from, as a URL. Many of
these coding problems are adapted “from the wild” and include the kinds
of context usually available to those answering questions.</li>
</ul>
<p>For the purposes of actually carrying out the initial evaluation,
we’re specifically interested in the <code>input</code> and
<code>target</code> columns. Let’s print out the first entry in full so
you can get a taste of a typical problem in this dataset:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">input</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; This bar chart shows the count of different cuts of diamonds, and each</span></span>
<span><span class="co">#&gt; bar is stacked and filled according to clarity:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; ggplot(data = diamonds) +</span></span>
<span><span class="co">#&gt; geom_bar(mapping = aes(x = cut, fill = clarity))</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Could you change this code so that the proportion of diamonds with a</span></span>
<span><span class="co">#&gt; given cut corresponds to the bar height and not the count? Each bar</span></span>
<span><span class="co">#&gt; should still be filled according to clarity.</span></span></code></pre>
<p>Here’s the suggested solution:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">target</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; Preferably:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; ggplot(data = diamonds) +</span></span>
<span><span class="co">#&gt; geom_bar(aes(x = cut, y = after_stat(count) / sum(after_stat(count)),</span></span>
<span><span class="co">#&gt; fill = clarity))</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0, but</span></span>
<span><span class="co">#&gt; it still works:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; ggplot(data = diamonds) +</span></span>
<span><span class="co">#&gt; geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill = clarity))</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Simply setting `position = "fill" will result in each bar having a</span></span>
<span><span class="co">#&gt; height of 1 and is not correct.</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="creating-and-evaluating-a-task">Creating and evaluating a task<a class="anchor" aria-label="anchor" href="#creating-and-evaluating-a-task"></a>
</h2>
<p>LLM evaluation with vitals happens in two main steps:</p>
<ol style="list-style-type: decimal">
<li>Use <code>Task$new()</code> to situate a dataset, solver, and scorer
in a <code>Task</code>.</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/Task.html">Task</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  dataset <span class="op">=</span> <span class="va">are</span>,</span>
<span>  solver <span class="op">=</span> <span class="fu"><a href="../reference/generate.html">generate</a></span><span class="op">(</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_anthropic.html" class="external-link">chat_anthropic</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"claude-3-7-sonnet-latest"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  scorer <span class="op">=</span> <span class="fu"><a href="../reference/scorer_model.html">model_graded_qa</a></span><span class="op">(</span>partial_credit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  name <span class="op">=</span> <span class="st">"An R Eval"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Use <code>Task$eval()</code> to evaluate the solver, evaluate the
scorer, and then explore a persistent log of the results in the
interactive Inspect log viewer.</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>After evaluation, the task contains information from the solving and
scoring steps. Here’s what the model responded to that first question
with:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">$</span><span class="va">samples</span><span class="op">$</span><span class="va">result</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; # Converting a stacked bar chart from counts to proportions</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; To change the bar heights from counts to proportions, you need to use</span></span>
<span><span class="co">#&gt; the `position = "fill"` argument in `geom_bar()`. This will normalize</span></span>
<span><span class="co">#&gt; each bar to represent proportions (with each bar having a total height</span></span>
<span><span class="co">#&gt; of 1), while maintaining the stacked clarity segments.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Here's the modified code:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ```r</span></span>
<span><span class="co">#&gt; ggplot(data = diamonds) +</span></span>
<span><span class="co">#&gt; geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill") +</span></span>
<span><span class="co">#&gt; labs(y = "Proportion") # Updating y-axis label to reflect the change</span></span>
<span><span class="co">#&gt; ```</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; This transformation:</span></span>
<span><span class="co">#&gt; - Maintains the stacking by clarity within each cut category</span></span>
<span><span class="co">#&gt; - Scales each bar to the same height (1.0)</span></span>
<span><span class="co">#&gt; - Shows the proportion of each clarity level within each cut</span></span>
<span><span class="co">#&gt; - Allows for easier comparison of clarity distributions across</span></span>
<span><span class="co">#&gt; different cuts</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The y-axis will now range from 0 to 1, representing the proportion</span></span>
<span><span class="co">#&gt; instead of raw counts.</span></span></code></pre>
<p>The task also contains score information from the scoring step. We’ve
used <code><a href="../reference/scorer_model.html">model_graded_qa()</a></code> as our scorer, which uses another
model to evaluate the quality of our solver’s solutions against the
reference solutions in the <code>target</code> column.
<code><a href="../reference/scorer_model.html">model_graded_qa()</a></code> is a model-graded scorer provided by the
package. This step compares Claude’s solutions against the reference
solutions in the <code>target</code> column, assigning a score to each
solution using another model. That score is either <code>1</code> or
<code>0</code>, though since we’ve set
<code>partial_credit = TRUE</code>, the model can also choose to allot
the response <code>.5</code>. vitals will use the same model that
generated the final response as the model to score solutions.</p>
<p>Hold up, though—we’re using an LLM to generate responses to
questions, and then using the LLM to grade those responses?</p>
<p><img src="https://cdn-useast1.kapwing.com/static/templates/3-spiderman-pointing-meme-template-full-ca8f27e0.webp" alt="The meme of 3 spiderman pointing at each other."></p>
<p>This technique is called “model grading” or “LLM-as-a-judge.” Done
correctly, model grading is an effective and scalable solution to
scoring. That said, it’s not without its faults. Here’s what the grading
model thought of the response:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">$</span><span class="va">samples</span><span class="op">$</span><span class="va">scorer_chat</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="fu">last_turn</span><span class="op">(</span><span class="op">)</span><span class="op">@</span><span class="va">text</span><span class="op">)</span></span>
<span><span class="co">#&gt; I need to assess whether the submission meets the criterion for</span></span>
<span><span class="co">#&gt; changing the bar chart to show proportions instead of counts.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The submission suggests using `position = "fill"` in the `geom_bar()`</span></span>
<span><span class="co">#&gt; function. This approach would normalize each bar to have a total height</span></span>
<span><span class="co">#&gt; of 1, showing the proportion of different clarity levels within each</span></span>
<span><span class="co">#&gt; cut category.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; However, the criterion specifies a different approach. According to the</span></span>
<span><span class="co">#&gt; criterion, each bar should represent the proportion of diamonds with a</span></span>
<span><span class="co">#&gt; given cut relative to the total number of diamonds. This means using</span></span>
<span><span class="co">#&gt; either:</span></span>
<span><span class="co">#&gt; 1. `aes(x = cut, y = after_stat(count) / sum(after_stat(count)), fill =</span></span>
<span><span class="co">#&gt; clarity)` (preferred modern syntax)</span></span>
<span><span class="co">#&gt; 2. `aes(x = cut, y = ..count.. / sum(..count..), fill = clarity)`</span></span>
<span><span class="co">#&gt; (deprecated but functional syntax)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The criterion explicitly states that using `position = "fill"` is not</span></span>
<span><span class="co">#&gt; correct because it would make each bar have a height of 1, rather than</span></span>
<span><span class="co">#&gt; showing the true proportion of each cut in the overall dataset.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The submission is suggesting a different type of proportional</span></span>
<span><span class="co">#&gt; representation than what is being asked for in the criterion. The</span></span>
<span><span class="co">#&gt; submission shows the distribution of clarity within each cut, while the</span></span>
<span><span class="co">#&gt; criterion is asking for the proportion of each cut in the total</span></span>
<span><span class="co">#&gt; dataset.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; GRADE: I</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="analyzing-the-results">Analyzing the results<a class="anchor" aria-label="anchor" href="#analyzing-the-results"></a>
</h2>
<p>Especially the first few times you run an eval, you’ll want to
inspect (ha!) its results closely. The vitals package ships with an app,
the Inspect log viewer, that allows you to drill down into the solutions
and grading decisions from each model for each sample. In the first
couple runs, you’ll likely find revisions you can make to your grading
guidance in <code>target</code> that align model responses with your
intent.</p>
<iframe src="../example-logs/vitals/index.html" width="100%" height="600px" style="border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);"></iframe>
<p><br></p>
<p>Under the hood, when you call <code>task$eval()</code>, results are
written to a <code>.json</code> file that the Inspect log viewer can
read. The Task object automatically launches the viewer when you call
<code>task$eval()</code> in an interactive session. You can also view
results any time with <code>are_task$view()</code>. You can explore this
eval above (on the package’s pkgdown site).</p>
<p>For a cursory analysis, we can start off by visualizing correct
vs. partially correct vs. incorrect answers:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/vitals_bind.html">vitals_bind</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task_data</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 26 × 4</span></span></span>
<span><span class="co">#&gt;    task     id                          score metadata         </span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                       <span style="color: #949494; font-style: italic;">&lt;ord&gt;</span> <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>           </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> are_task after-stat-bar-heights      I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> are_task conditional-grouped-summary P     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> are_task correlated-delays-reasoning C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> are_task curl-http-get               C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> are_task dropped-level-legend        I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> are_task geocode-req-perform         C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> are_task ggplot-breaks-feature       I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> are_task grouped-filter-summarize    P     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> are_task grouped-mutate              C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> are_task implement-nse-arg           P     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 16 more rows</span></span></span>
<span></span>
<span><span class="va">are_task_data</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">score</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p><img src="vitals_files/figure-html/plot-1-1.png" alt="A ggplot2 bar plot, showing Claude was correct most of the time." width="700"></p>
<p>Claude answered fully correctly in 14 out of 26 samples, and
partially correctly 6 times.For me, this leads to all sorts of
questions:</p>
<ul>
<li>Are there any models that are cheaper than Claude that would do just
as well? Or even a local model?</li>
<li>Are there other models available that would do better out of the
box?</li>
<li>Would Claude do better if I allow it to “reason” briefly before
answering?</li>
<li>Would Claude do better if I gave it tools that’d allow it to peruse
documentation and/or run R code before answering? (See <a href="https://posit-dev.github.io/btw/reference/btw_register_tools.html" class="external-link"><code>btw::btw_register_tools()</code></a>
if you’re interested in this.)</li>
</ul>
<p>These questions can be explored by evaluating Tasks against different
solvers and scorers. For example, to compare Claude’s performance with
OpenAI’s GPT-4o, we just need to clone the object and then run
<code>$eval()</code> with a different solver <code>chat</code>:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_openai</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_task_openai</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html" class="external-link">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Any arguments to solving or scoring functions can be passed directly
to <code>$eval()</code>, allowing for quickly evaluating tasks across
several parameterizations.</p>
<p>Using this data, we can quickly juxtapose those evaluation
results:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_eval</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="../reference/vitals_bind.html">vitals_bind</a></span><span class="op">(</span><span class="va">are_task</span>, <span class="va">are_task_openai</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span></span>
<span>    task <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html" class="external-link">if_else</a></span><span class="op">(</span><span class="va">task</span> <span class="op">==</span> <span class="st">"are_task"</span>, <span class="st">"Claude"</span>, <span class="st">"GPT-4o"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html" class="external-link">rename</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">task</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task_eval</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span></span>
<span>    score <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html" class="external-link">case_when</a></span><span class="op">(</span></span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"I"</span> <span class="op">~</span> <span class="st">"Incorrect"</span>,</span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"P"</span> <span class="op">~</span> <span class="st">"Partially correct"</span>,</span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"C"</span> <span class="op">~</span> <span class="st">"Correct"</span></span>
<span>      <span class="op">)</span>,</span>
<span>      levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Incorrect"</span>, <span class="st">"Partially correct"</span>, <span class="st">"Correct"</span><span class="op">)</span>,</span>
<span>      ordered <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">model</span>, fill <span class="op">=</span> <span class="va">score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_brewer.html" class="external-link">scale_fill_brewer</a></span><span class="op">(</span>breaks <span class="op">=</span> <span class="va">rev</span>, palette <span class="op">=</span> <span class="st">"RdYlGn"</span><span class="op">)</span></span></code></pre></div>
<p><img src="vitals_files/figure-html/unnamed-chunk-4-1.png" width="700"></p>
<p>Is this difference in performance just a result of noise, though? We
can supply the scores to an ordinal regression model to answer this
question.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/runehaubo/ordinal" class="external-link">ordinal</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'ordinal'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:dplyr':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     slice</span></span>
<span></span>
<span><span class="va">are_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ordinal/man/clm.html" class="external-link">clm</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">model</span>, data <span class="op">=</span> <span class="va">are_task_eval</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_mod</span></span>
<span><span class="co">#&gt; formula: score ~ model</span></span>
<span><span class="co">#&gt; data:    are_task_eval</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  link  threshold nobs logLik AIC    niter max.grad cond.H </span></span>
<span><span class="co">#&gt;  logit flexible  52   -54.80 115.61 4(0)  2.14e-12 1.2e+01</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt; modelGPT-4o </span></span>
<span><span class="co">#&gt;     -0.9725 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Threshold coefficients:</span></span>
<span><span class="co">#&gt;      I|P      P|C </span></span>
<span><span class="co">#&gt; -1.35956 -0.09041</span></span></code></pre></div>
<p>The coefficient for <code>model == "GPT-4o"</code> is -0.972,
indicating that GPT-4o tends to be associated with lower grades. If a
95% confidence interval for this coefficient contains zero, we can
conclude that there is not sufficient evidence to reject the null
hypothesis that the difference between GPT-4o and Claude’s performance
on this eval is zero at the 0.05 significance level.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">are_mod</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 2.5 %     97.5 %</span></span>
<span><span class="co">#&gt; modelGPT-4o -2.031694 0.04806051</span></span></code></pre></div>
<div class="callout-note">
<p>If we had evaluated this model across multiple epochs, the question
ID could become a “nuisance parameter” in a mixed model, e.g. with the
model structure
<code>ordinal::clmm(score ~ model + (1|id), ...)</code>.</p>
</div>
<p>This vignette demonstrated the simplest possible evaluation based on
the <code>are</code> dataset. If you’re interested in carrying out more
advanced evals, check out the other vignettes in this package!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Simon Couch, Posit Software, PBC.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
