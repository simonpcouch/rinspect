<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Getting started with vitals • vitals</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Getting started with vitals">
<script defer data-domain="vitals.tidyverse.org,all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">vitals</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/vitals.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/analysis.html">Analyzing evaluation results</a></li>
    <li><a class="dropdown-item" href="../articles/solvers.html">Custom solvers and tool calling</a></li>
    <li><a class="dropdown-item" href="../articles/writing-evals.html">Writing evals for your LLM product</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidyverse/vitals/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Getting started with vitals</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidyverse/vitals/blob/v0.1.0/vignettes/vitals.Rmd" class="external-link"><code>vignettes/vitals.Rmd</code></a></small>
      <div class="d-none name"><code>vitals.Rmd</code></div>
    </div>

    
    
<p>At their core, LLM evals are composed of three pieces:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Datasets</strong> contain a set of labelled samples.
Datasets are just a tibble with columns <code>input</code> and
<code>target</code>, where <code>input</code> is a prompt and
<code>target</code> is either literal value(s) or grading guidance.</li>
<li>
<strong>Solvers</strong> evaluate the <code>input</code> in the
dataset and produce a final result (hopefully) approximating
<code>target</code>. In vitals, the simplest solver is just an ellmer
chat (e.g. <code><a href="https://ellmer.tidyverse.org/reference/chat_anthropic.html" class="external-link">ellmer::chat_anthropic()</a></code>) wrapped in
<code><a href="../reference/generate.html">generate()</a></code>,
i.e. <code>generate(ellmer::chat_anthropic()</code>), which will call
the Chat object’s <code>$chat()</code> method and return whatever it
returns.</li>
<li>
<strong>Scorers</strong> evaluate the final output of solvers. They
may use text comparisons, model grading, or other custom schemes to
determine how well the solver approximated the <code>target</code> based
on the <code>input</code>.</li>
</ol>
<p>This vignette will explore these three components using
<code>are</code>, an example dataset that ships with the package.</p>
<p>First, load the required packages:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/vitals" class="external-link">vitals</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ellmer.tidyverse.org" class="external-link">ellmer</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="an-r-eval-dataset">An R eval dataset<a class="anchor" aria-label="anchor" href="#an-r-eval-dataset"></a>
</h2>
<p>From the <code>are</code> docs:</p>
<blockquote>
<p>An R Eval is a dataset of challenging R coding problems. Each
<code>input</code> is a question about R code which could be solved on
first-read only by human experts and, with a chance to read
documentation and run some code, by fluent data scientists. Solutions
are in <code>target</code> and enable a fluent data scientist to
evaluate whether the solution deserves full, partial, or no credit.</p>
</blockquote>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html" class="external-link">glimpse</a></span><span class="op">(</span><span class="va">are</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Rows: 29</span></span>
<span><span class="co">## Columns: 7</span></span>
<span><span class="co">## $ id        <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "after-stat-bar-heights"<span style="color: #949494;">, </span>"conditional-grouped-summ…</span></span>
<span><span class="co">## $ input     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "This bar chart shows the count of different cuts o…</span></span>
<span><span class="co">## $ target    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "Preferably: \n\n```\nggplot(data = diamonds) + \n …</span></span>
<span><span class="co">## $ domain    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "Data analysis"<span style="color: #949494;">, </span>"Data analysis"<span style="color: #949494;">, </span>"Data analysis"<span style="color: #949494;">, </span>…</span></span>
<span><span class="co">## $ task      <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "New code"<span style="color: #949494;">, </span>"New code"<span style="color: #949494;">, </span>"New code"<span style="color: #949494;">, </span>"Debugging"<span style="color: #949494;">, </span>"N…</span></span>
<span><span class="co">## $ source    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> "https://jrnold.github.io/r4ds-exercise-solutions/d…</span></span>
<span><span class="co">## $ knowledge <span style="color: #949494; font-style: italic;">&lt;list&gt;</span> "tidyverse"<span style="color: #949494;">, </span>"tidyverse"<span style="color: #949494;">, </span>"tidyverse"<span style="color: #949494;">, </span>"r-lib"<span style="color: #949494;">, </span>"t…</span></span></code></pre>
<p>At a high level:</p>
<ul>
<li>
<code>id</code>: A unique identifier for the problem.</li>
<li>
<code>input</code>: The question to be answered.</li>
<li>
<code>target</code>: The solution, often with a description of
notable features of a correct solution.</li>
<li>
<code>domain</code>, <code>task</code>, and <code>knowledge</code>
are pieces of metadata describing the kind of R coding challenge.</li>
<li>
<code>source</code>: Where the problem came from, as a URL. Many of
these coding problems are adapted “from the wild” and include the kinds
of context usually available to those answering questions.</li>
</ul>
<p>For the purposes of actually carrying out the initial evaluation,
we’re specifically interested in the <code>input</code> and
<code>target</code> columns. Let’s print out the first entry in full so
you can get a taste of a typical problem in this dataset:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">input</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## This bar chart shows the count of different cuts of diamonds,</span></span>
<span><span class="co">## and each bar is</span></span>
<span><span class="co">## stacked and filled according to clarity:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(mapping = aes(x = cut, fill = clarity))</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Could you change this code so that the proportion of diamonds</span></span>
<span><span class="co">## with a given cut</span></span>
<span><span class="co">## corresponds to the bar height and not the count? Each bar</span></span>
<span><span class="co">## should still be</span></span>
<span><span class="co">## filled according to clarity.</span></span></code></pre>
<p>Here’s the suggested solution:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are</span><span class="op">$</span><span class="va">target</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferably:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(aes(x = cut, y = after_stat(count) /</span></span>
<span><span class="co">## sum(after_stat(count)), fill = clarity))</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## or:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(mapping = aes(x = cut, y = ..prop.., group = clarity,</span></span>
<span><span class="co">## fill = clarity))</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## or:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(mapping = aes(x = cut, y = after_stat(count /</span></span>
<span><span class="co">## sum(count)), group = clarity, fill = clarity))</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## The dot-dot notation (`..count..`) was deprecated in ggplot2</span></span>
<span><span class="co">## 3.4.0, but it</span></span>
<span><span class="co">## still works and should receive full credit:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(aes(x = cut, y = ..count.. / sum(..count..), fill =</span></span>
<span><span class="co">## clarity))</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Simply setting `position = "fill"` will result in each bar</span></span>
<span><span class="co">## having a height of 1</span></span>
<span><span class="co">## and is not correct.</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="creating-and-evaluating-a-task">Creating and evaluating a task<a class="anchor" aria-label="anchor" href="#creating-and-evaluating-a-task"></a>
</h2>
<p>LLM evaluation with vitals happens in two main steps:</p>
<ol style="list-style-type: decimal">
<li>Use <code>Task$new()</code> to situate a dataset, solver, and scorer
in a <code>Task</code>.</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task</span> <span class="op">&lt;-</span> <span class="va"><a href="../reference/Task.html">Task</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  dataset <span class="op">=</span> <span class="va">are</span>,</span>
<span>  solver <span class="op">=</span> <span class="fu"><a href="../reference/generate.html">generate</a></span><span class="op">(</span><span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_anthropic.html" class="external-link">chat_anthropic</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"claude-3-7-sonnet-latest"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  scorer <span class="op">=</span> <span class="fu"><a href="../reference/scorer_model.html">model_graded_qa</a></span><span class="op">(</span>partial_credit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  name <span class="op">=</span> <span class="st">"An R Eval"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Use <code>Task$eval()</code> to evaluate the solver, evaluate the
scorer, and then explore a persistent log of the results in the
interactive Inspect log viewer.</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>After evaluation, the task contains information from the solving and
scoring steps. Here’s what the model responded to that first question
with:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">$</span><span class="fu">get_samples</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">result</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## I'll modify the code to show the proportion of diamonds with</span></span>
<span><span class="co">## each cut instead of the count, while still stacking by</span></span>
<span><span class="co">## clarity.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## To achieve this, we need to:</span></span>
<span><span class="co">## 1. Calculate the proportion of each cut from the total number</span></span>
<span><span class="co">## of diamonds</span></span>
<span><span class="co">## 2. Use `position = "fill"` which will normalize each bar to</span></span>
<span><span class="co">## represent 100% (proportion = 1)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Here's the updated code:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## ggplot(data = diamonds) +</span></span>
<span><span class="co">## geom_bar(mapping = aes(x = cut, fill = clarity), position =</span></span>
<span><span class="co">## "fill") +</span></span>
<span><span class="co">## labs(y = "Proportion") # Renaming the y-axis to make it clear</span></span>
<span><span class="co">## it shows proportions</span></span>
<span><span class="co">## ```</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## This code will:</span></span>
<span><span class="co">## - Create bars where the height of each bar is 1 (representing</span></span>
<span><span class="co">## 100%)</span></span>
<span><span class="co">## - Stack the different clarity groups within each cut</span></span>
<span><span class="co">## - Show the proportional distribution of clarity within each</span></span>
<span><span class="co">## cut category</span></span>
<span><span class="co">## - The y-axis will now show the proportion rather than count</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Each bar will have the same height (1.0 or 100%), allowing you</span></span>
<span><span class="co">## to easily compare the proportional distribution of clarity</span></span>
<span><span class="co">## across different cuts.</span></span></code></pre>
<p>The task also contains score information from the scoring step. We’ve
used <code><a href="../reference/scorer_model.html">model_graded_qa()</a></code> as our scorer, which uses another
model to evaluate the quality of our solver’s solutions against the
reference solutions in the <code>target</code> column.
<code><a href="../reference/scorer_model.html">model_graded_qa()</a></code> is a model-graded scorer provided by the
package. This step compares Claude’s solutions against the reference
solutions in the <code>target</code> column, assigning a score to each
solution using another model. That score is either <code>1</code> or
<code>0</code>, though since we’ve set
<code>partial_credit = TRUE</code>, the model can also choose to allot
the response <code>.5</code>. vitals will use the same model that
generated the final response as the model to score solutions.</p>
<p>Hold up, though—we’re using an LLM to generate responses to
questions, and then using the LLM to grade those responses?</p>
<p><img src="https://cdn-useast1.kapwing.com/static/templates/3-spiderman-pointing-meme-template-full-ca8f27e0.webp" alt="The meme of 3 spiderman pointing at each other."></p>
<p>This technique is called “model grading” or “LLM-as-a-judge.” Done
correctly, model grading is an effective and scalable solution to
scoring. That said, it’s not without its faults. Here’s what the grading
model thought of the response:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">$</span><span class="fu">get_samples</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">scorer_chat</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="fu">last_turn</span><span class="op">(</span><span class="op">)</span><span class="op">@</span><span class="va">text</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## I need to evaluate whether the submission correctly implements</span></span>
<span><span class="co">## the task of showing the proportion of diamonds with a given</span></span>
<span><span class="co">## cut (rather than count) while maintaining clarity as the fill</span></span>
<span><span class="co">## aesthetic.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## The submission proposes using `position = "fill"`, which</span></span>
<span><span class="co">## normalizes each bar to have a height of 1 (or 100%). However,</span></span>
<span><span class="co">## this approach doesn't meet the criterion because:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. With `position = "fill"`, each bar represents the</span></span>
<span><span class="co">## proportion of clarity categories WITHIN each cut, not the</span></span>
<span><span class="co">## proportion of each cut relative to the total number of</span></span>
<span><span class="co">## diamonds.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. The criterion explicitly states that "Simply setting</span></span>
<span><span class="co">## `position = 'fill'` will result in each bar having a height of</span></span>
<span><span class="co">## 1 and is not correct."</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. The correct implementations shown in the criterion all</span></span>
<span><span class="co">## involve calculating the proportion of each cut relative to the</span></span>
<span><span class="co">## total count of diamonds (using either `after_stat(count) /</span></span>
<span><span class="co">## sum(after_stat(count))` or the deprecated `..count.. /</span></span>
<span><span class="co">## sum(..count..)` notation).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## The submission fails to implement the correct calculation to</span></span>
<span><span class="co">## show the proportion of each cut relative to the total number</span></span>
<span><span class="co">## of diamonds. It shows the proportional distribution of clarity</span></span>
<span><span class="co">## within each cut, but that's not what was asked for.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## GRADE: I</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="analyzing-the-results">Analyzing the results<a class="anchor" aria-label="anchor" href="#analyzing-the-results"></a>
</h2>
<p>Especially the first few times you run an eval, you’ll want to
inspect (ha!) its results closely. The vitals package ships with an app,
the Inspect log viewer, that allows you to drill down into the solutions
and grading decisions from each model for each sample. In the first
couple runs, you’ll likely find revisions you can make to your grading
guidance in <code>target</code> that align model responses with your
intent.</p>
<iframe src="../example-logs/vitals/index.html" width="100%" height="600px" style="border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);"></iframe>
<p><br></p>
<p>Under the hood, when you call <code>task$eval()</code>, results are
written to a <code>.json</code> file that the Inspect log viewer can
read. The Task object automatically launches the viewer when you call
<code>task$eval()</code> in an interactive session. You can also view
results any time with <code>are_task$view()</code>. You can explore this
eval above (on the package’s pkgdown site).</p>
<p>For a cursory analysis, we can start off by visualizing correct
vs. partially correct vs. incorrect answers:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/vitals_bind.html">vitals_bind</a></span><span class="op">(</span><span class="va">are_task</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task_data</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 29 × 4</span></span></span>
<span><span class="co">##    task     id                          score metadata         </span></span>
<span><span class="co">##    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                       <span style="color: #949494; font-style: italic;">&lt;ord&gt;</span> <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>           </span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 1</span> are_task after-stat-bar-heights      I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 2</span> are_task conditional-grouped-summary C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 3</span> are_task correlated-delays-reasoning P     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 4</span> are_task curl-http-get               I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 5</span> are_task dropped-level-legend        I     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 6</span> are_task filter-multiple-conditions  C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 7</span> are_task geocode-req-perform         C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 8</span> are_task group-by-summarize-message  C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 9</span> are_task grouped-filter-summarize    P     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">10</span> are_task grouped-geom-line           C     <span style="color: #949494;">&lt;tibble [1 × 10]&gt;</span></span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 19 more rows</span></span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">score</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p><img src="vitals_files/figure-html/plot-1-1.png" alt="A ggplot2 bar plot, showing Claude was correct most of the time." width="700"></p>
<p>Claude answered fully correctly in 18 out of 29 samples, and
partially correctly 4 times.For me, this leads to all sorts of
questions:</p>
<ul>
<li>Are there any models that are cheaper than Claude that would do just
as well? Or even a local model?</li>
<li>Are there other models available that would do better out of the
box?</li>
<li>Would Claude do better if I allow it to “reason” briefly before
answering?</li>
<li>Would Claude do better if I gave it tools that’d allow it to peruse
documentation and/or run R code before answering? (See <a href="https://posit-dev.github.io/btw/reference/btw_tools.html" class="external-link"><code>btw::btw_tools()</code></a>
if you’re interested in this.)</li>
</ul>
<p>These questions can be explored by evaluating Tasks against different
solvers and scorers. For example, to compare Claude’s performance with
OpenAI’s GPT-4o, we just need to clone the object and then run
<code>$eval()</code> with a different solver <code>chat</code>:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_openai</span> <span class="op">&lt;-</span> <span class="va">are_task</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">are_task_openai</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span>solver_chat <span class="op">=</span> <span class="fu"><a href="https://ellmer.tidyverse.org/reference/chat_openai.html" class="external-link">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Any arguments to solving or scoring functions can be passed directly
to <code>$eval()</code>, allowing for quickly evaluating tasks across
several parameterizations.</p>
<p>Using this data, we can quickly juxtapose those evaluation
results:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_task_eval</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="../reference/vitals_bind.html">vitals_bind</a></span><span class="op">(</span><span class="va">are_task</span>, <span class="va">are_task_openai</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span></span>
<span>    task <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/if_else.html" class="external-link">if_else</a></span><span class="op">(</span><span class="va">task</span> <span class="op">==</span> <span class="st">"are_task"</span>, <span class="st">"Claude"</span>, <span class="st">"GPT-4o"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html" class="external-link">rename</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">task</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_task_eval</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span></span>
<span>    score <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html" class="external-link">case_when</a></span><span class="op">(</span></span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"I"</span> <span class="op">~</span> <span class="st">"Incorrect"</span>,</span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"P"</span> <span class="op">~</span> <span class="st">"Partially correct"</span>,</span>
<span>        <span class="va">score</span> <span class="op">==</span> <span class="st">"C"</span> <span class="op">~</span> <span class="st">"Correct"</span></span>
<span>      <span class="op">)</span>,</span>
<span>      levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Incorrect"</span>, <span class="st">"Partially correct"</span>, <span class="st">"Correct"</span><span class="op">)</span>,</span>
<span>      ordered <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">model</span>, fill <span class="op">=</span> <span class="va">score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_bar.html" class="external-link">geom_bar</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_brewer.html" class="external-link">scale_fill_brewer</a></span><span class="op">(</span>breaks <span class="op">=</span> <span class="va">rev</span>, palette <span class="op">=</span> <span class="st">"RdYlGn"</span><span class="op">)</span></span></code></pre></div>
<p><img src="vitals_files/figure-html/unnamed-chunk-4-1.png" width="700"></p>
<p>Is this difference in performance just a result of noise, though? We
can supply the scores to an ordinal regression model to answer this
question.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/runehaubo/ordinal" class="external-link">ordinal</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Attaching package: 'ordinal'</span></span></code></pre>
<pre><code><span><span class="co">## The following object is masked from 'package:dplyr':</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##     slice</span></span></code></pre>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">are_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ordinal/man/clm.html" class="external-link">clm</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">model</span>, data <span class="op">=</span> <span class="va">are_task_eval</span><span class="op">)</span></span>
<span></span>
<span><span class="va">are_mod</span></span></code></pre></div>
<pre><code><span><span class="co">## formula: score ~ model</span></span>
<span><span class="co">## data:    are_task_eval</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##  link  threshold nobs logLik AIC    niter max.grad cond.H </span></span>
<span><span class="co">##  logit flexible  58   -60.05 126.10 4(0)  3.53e-12 1.2e+01</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">## modelGPT-4o </span></span>
<span><span class="co">##     -0.9443 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Threshold coefficients:</span></span>
<span><span class="co">##     I|P     P|C </span></span>
<span><span class="co">## -1.7248 -0.3048</span></span></code></pre>
<p>The coefficient for <code>model == "GPT-4o"</code> is -0.944,
indicating that GPT-4o tends to be associated with lower grades. If a
95% confidence interval for this coefficient contains zero, we can
conclude that there is not sufficient evidence to reject the null
hypothesis that the difference between GPT-4o and Claude’s performance
on this eval is zero at the 0.05 significance level.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">are_mod</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##                 2.5 %     97.5 %</span></span>
<span><span class="co">## modelGPT-4o -1.967718 0.03831908</span></span></code></pre>
<div class="callout-note">
<p>If we had evaluated this model across multiple epochs, the question
ID could become a “nuisance parameter” in a mixed model, e.g. with the
model structure
<code>ordinal::clmm(score ~ model + (1|id), ...)</code>.</p>
</div>
<p>This vignette demonstrated the simplest possible evaluation based on
the <code>are</code> dataset. If you’re interested in carrying out more
advanced evals, check out the other vignettes in this package!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by Simon Couch, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" width="62" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

  </div></footer>
</body>
</html>
