<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Writing evals for your LLM product • vitals</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Writing evals for your LLM product">
<script defer data-domain="vitals.tidyverse.org,all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">vitals</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Unreleased version">0.0.0.9002</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/vitals.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/solvers.html">Custom solvers and tool calling</a></li>
    <li><a class="dropdown-item" href="../articles/writing-evals.html">Writing evals for your LLM product</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidyverse/vitals/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article" id="container">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Writing evals for your LLM product</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/tidyverse/vitals/blob/main/vignettes/writing-evals.Rmd" class="external-link"><code>vignettes/writing-evals.Rmd</code></a></small>
      <div class="d-none name"><code>writing-evals.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/vitals" class="external-link">vitals</a></span><span class="op">)</span></span></code></pre></div>
<p>Writing evaluations for LLM products is hard. It takes a <em>lot</em>
of effort to design a system that effectively identifies value and
surfaces issues. So, why do so?</p>
<p>First, <strong>public evals will not cut it.</strong> The release
posts for new language models often include tables displaying results
from several of the most popular public evaluations. For example, this
table from the <a href="https://www.anthropic.com/news/claude-4" class="external-link">Claude
4 release post</a>:</p>
<p><img src="figures/claude_4.png" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.3);"></p>
<p>These widely used, off-the-shelf evals measure various capabilities
of <em>models</em> like real-world GitHub issue resolution, tool use,
etc. While these public evaluations serve as a good reference point for
choosing which models to experiment with initially, they’re not a good
measure of how well <em>your product</em> specifically performs. As
<span class="citation">Yan (2024)</span> writes, “If you’ve ran
off-the-shelf evals for your tasks, you may have found that most don’t
work. They barely correlate with application-specific performance and
aren’t discriminative enough to use in production.” When designing some
LLM product, your task is to evaluate that product rather than the model
underlying it.</p>
<p>Does an LLM product really need its own evaluation, though?
Especially in early development stages, couldn’t the product builder
and/or domain expert just test it out themselves, interactively? While
this works in the earliest stages of experimentation, <strong>you can’t
get by with not running evals</strong> if you want your product to go to
production. As applications develop from prototypes into production
systems, keeping up with all of the potential points of failure while
trying to iterate can become unsustainable without sufficient
automation: “Addressing one failure mode led to the emergence of others,
resembling a game of whack-a-mole” <span class="citation">(Husain
2024b)</span>. Thus, without automated evals, as you otherwise get
closer and closer to production-readiness, the pace at which you can
iterate on your product slows to a halt. In contrast, “If you streamline
your evaluation process, all other activities become easy. This is very
similar to how tests in software engineering pay massive dividends in
the long term despite requiring up-front investment” <span class="citation">(Husain 2024b)</span>.</p>
<p>In short, custom evals are necessary to bring your LLM product from
demo to production.</p>
<div class="section level2">
<h2 id="how-to-write-good-evals">How to write good evals<a class="anchor" aria-label="anchor" href="#how-to-write-good-evals"></a>
</h2>
<p>Writing an eval with vitals requires defining a dataset, a solver,
and scorer. I’ll speak to each of these elements individually.</p>
<div class="section level3">
<h3 id="datasets">Datasets<a class="anchor" aria-label="anchor" href="#datasets"></a>
</h3>
<p>In vitals, datasets are a data frame with columns, minimally,
<code>input</code> and <code>target</code>. A “sample” is a row of a
dataset. <code>input</code> defines questions inputted by end users and
<code>target</code> defines the target answer and/or grading guidance
for it. What sorts of input prompts should you include, though?</p>
<p>In short, inputs should be <strong>natural</strong>. Rather than
“setting up” the model with exactly the right context and phrasing,
“[i]t’s important that the dataset… represents the types of interactions
that your AI will have in production” <span class="citation">(Husain
2024a)</span>.</p>
<p>If your system is going to answer a set of questions similar to some
set that already exists—support tickets, for example—use the actual
tickets themselves rather than writing your own from scratch. In this
case, refrain from correcting spelling errors, removing unneeded
context, or doing any “sanitizing” before providing the system with the
input; you want the distribution of inputs to resemble what the system
will encounter in the wild as closely as possible.</p>
<p>If there is no existing resource of input prompts to pull from, still
try to avoid this sort of unrealistic set-up. I’ll specifically call out
multiple choice questions here—while multiple choice responses are easy
to grade automatically, your inputs should only provide a system with
multiple choices to select from if the production system will also have
access to multiple choices <span class="citation">(Press 2024)</span>.
If you’re writing your own questions, I encourage you to read the <a href="https://hamel.dev/blog/posts/llm-judge/#dimensions-for-structuring-your-dataset" class="external-link">“Dimensions
for Structuring Your Dataset” section</a> from <span class="citation">Husain (2024a)</span>, which provides a few axes to
keep in mind when thinking about how to generate data that resembles
what your system will ultimately see:</p>
<blockquote>
<p>You want to define dimensions that make sense for your use case. For
example, here are ones that I often use for B2C applications:</p>
<p>Features: Specific functionalities of your AI product.</p>
<p>Scenarios: Situations or problems the AI may encounter and needs to
handle.</p>
<p>Personas: Representative user profiles with distinct characteristics
and needs.</p>
</blockquote>
<p>The other part of the “how” is the mechanics. You probably don’t want
to paste in a bunch of questions into a call to <code><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble()</a></code>,
escape a bunch of quotes, etc. Instead, here’s how I’ve written
evals:</p>
<ul>
<li>Consider what metadata you want to keep track of along with your
<code>input</code> and <code>target</code>. Do you want to tag questions
with categories? Include a source URL? This isn’t stuff that will
actually be integrated into the eval, but that will be carried along
throughout the evaluation to aid you with analysis down the line.</li>
<li>Prompt a model (Claude 4 Sonnet is quite good at this) to make you a
Shiny app that will help you generate samples quickly (and, hopefully,
joyfully). The app might provide you with a free text box to paste in
the <code>input</code> and another for the <code>target</code>, fields
for whatever metadata you’d like to carry along, and a “Submit” button.
When you click “Submit”, the app should write the inputted values into a
<code>.yaml</code> file in some directory and “clear” the app so that
you can then add a new sample. Here’s a <a href="https://github.com/simonpcouch/choreseval/blob/main/data-raw/sample-generator-prompt.txt" class="external-link">sample
prompt</a> that I used to help me write an eval dataset recently.</li>
<li>Then, once you’ve created a sufficient number of samples (30 is a
good place to start), read all of the yaml files in that directory into
R, bind rows, and you’ve got a dataset. Here’s a <a href="https://github.com/simonpcouch/choreseval/blob/51694ad44f09fdef234f7348f4409c494c585bd3/data-raw/chores-dataset.R" class="external-link">sample
script</a> that does this for the eval linked in the previous
bullet.</li>
</ul>
</div>
<div class="section level3">
<h3 id="solvers">Solvers<a class="anchor" aria-label="anchor" href="#solvers"></a>
</h3>
<p>The solver is your AI product itself. If the product is a chat app or
an app that integrates with a chat feature, you can supply the ellmer
Chat object powering the chat directly as the <code>solver_chat</code>
argument to <code><a href="../reference/generate.html">generate()</a></code> and you’re ready to go. In that
case, ensure that the same prompt and tools are all attached to the Chat
object as they would be in your product.</p>
</div>
<div class="section level3">
<h3 id="scorers">Scorers<a class="anchor" aria-label="anchor" href="#scorers"></a>
</h3>
<p>Scorers take the <code>input</code>, <code>target</code>, and the
solver’s output, and deliver some judgment about whether the solver’s
output satisfies the grading guidance from <code>target</code> closely
enough. At the most fundamental level, LLMs are useful in situations
where inputs and outputs are diverse and considerably variable in
structure; as a result, determining correctness in evals is a hard
problem.</p>
<p>When implementing a scorer, you have a few options</p>
<ul>
<li>Deterministic scoring: In some specific situations, it’s possible to
write code that can check correctness without any use of an LLM. For
example, if your solver chooses an answer from multiple choices, you
could just use a regular expression (as in the scorers
<code><a href="../reference/scorer_detect.html">detect_match()</a></code> and friends). Or, if your scorer outputs
code, you could check if it compiles/parses and then run that code
through a unit test.</li>
<li>LLM-as-a-judge: In most situations, you’ll want to pass the input,
the solver’s result, and the grading guidance (<code>target</code>) to a
model, and ask the model to assign some score. (vitals natively supports
factors with levels <code>I &lt; P &lt; C</code>, standing for
Incorrect, (optional) Partially Correct, and Correct.)</li>
<li>Human grading: In the earliest (before you even develop a scorer)
and latest (right before your product goes to production) stages of
product development, you should be spending a lot of time looking at the
solver’s outputs yourself.</li>
</ul>
<table class="table">
<colgroup>
<col width="25%">
<col width="25%">
<col width="25%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>Technique</th>
<th>Deterministic Scoring</th>
<th>LLM-as-a-judge / model grading</th>
<th>Human grading</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Speed</td>
<td>Very fast</td>
<td>Pretty fast</td>
<td>Very slow</td>
</tr>
<tr class="even">
<td>Cost</td>
<td>Very cheap / “free”</td>
<td>Pretty cheap</td>
<td>Very expensive</td>
</tr>
<tr class="odd">
<td>Applicability</td>
<td>Narrow</td>
<td>Broad</td>
<td>Broad</td>
</tr>
</tbody>
</table>
<p>Understandably, many people bristle at the thought of LLMs evaluating
their own output; while these systems do take some careful refinement,
“when implemented well, LLM-as-Judge achieves decent correlation with
human judgments” <span class="citation">(Yan et al. 2024)</span>. While
vitals will safeguard you from many common LLM-as-a-judge missteps with
<code><a href="../reference/scorer_model.html">model_graded_qa()</a></code>, some recommendations on implementing
these systems for your own use cases:</p>
<ul>
<li><p><strong>You should still be looking at lots of data
yourself</strong>: “After looking at the data, it’s likely you will find
errors in your AI system. Instead of plowing ahead and building an LLM
judge, you want to fix any obvious errors. Remember, the whole point of
the LLM as a judge is to help you find these errors, so it’s totally
fine if you find them earlier” <span class="citation">(Husain
2024a)</span>. Especially when you first implement the LLM-as-a-judge,
look at its reasoning output and ensure that it aligns with your own
intuitions as closely as possible. Sometimes this means fixing the
judge, and sometimes it means going back and editing grading guidance in
the samples themselves.</p></li>
<li>
<p><strong>Start with binary judgments</strong>: It may be tempting
to implement some sort of scoring rubric that assigns a numeric score to
output. Instead, start off by trying to align a scorer that only returns
Pass/Fail with your own intuitions about whether an output is
satisfactory. With the right <code>instructions</code> argument, you
might be able to get <code><a href="../reference/scorer_model.html">model_graded_qa()</a></code> to output trusted
judgments reliably.</p>
<ul>
<li>One exception here is if you’re measuring some product’s ability to
follow a set of instructions. The <a href="https://github.com/simonpcouch/choreseval" class="external-link">chores eval</a> is one
example of this, where the scorer assigns a binary judgment to a set of
scoring items, and then R code takes those judgments and sums them to
create a numeric score.</li>
</ul>
</li>
<li><p><strong>Use strong models to judge</strong>: In your production
system, you might be interested in driving down costs and reducing
latency by choosing smaller models in your solver. In your
LLM-as-a-judge system, though, stick with “flagship” models like Claude
4 Sonnet, GPT 4.1, Gemini 2.5 Pro, etc.; you’re much more likely to get
results you can begin to trust from larger models.</p></li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-husain2024judge" class="csl-entry">
Husain, Hamel. 2024a. <span>“Creating a LLM-as-a-Judge That Drives
Business Results.”</span> <a href="https://hamel.dev/blog/posts/llm-judge/index.html" class="external-link">https://hamel.dev/blog/posts/llm-judge/index.html</a>.
</div>
<div id="ref-husain2024evals" class="csl-entry">
———. 2024b. <span>“Your AI Product Needs Evals.”</span> <a href="https://hamel.dev/blog/posts/evals/" class="external-link">https://hamel.dev/blog/posts/evals/</a>.
</div>
<div id="ref-press2024benchmarks" class="csl-entry">
Press, Ofir. 2024. <span>“How to Build Good Language Modeling
Benchmarks.”</span> <a href="https://ofir.io/How-to-Build-Good-Language-Modeling-Benchmarks/" class="external-link">https://ofir.io/How-to-Build-Good-Language-Modeling-Benchmarks/</a>.
</div>
<div id="ref-yan2024evals" class="csl-entry">
Yan, Eugene. 2024. <span>“Task-Specific LLM Evals That Do &amp; Don’t
Work.”</span> <em>Eugeneyan.com</em>, March. <a href="https://eugeneyan.com/writing/evals/" class="external-link">https://eugeneyan.com/writing/evals/</a>.
</div>
<div id="ref-yan2024building" class="csl-entry">
Yan, Eugene, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu, and
Shreya Shankar. 2024. <span>“What We’ve Learned from a Year of Building
with LLMs.”</span> <em>Applied LLMs</em>, June. <a href="https://applied-llms.org/" class="external-link">https://applied-llms.org/</a>.
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by Simon Couch, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" width="62" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

  </div></footer>
</body>
</html>
